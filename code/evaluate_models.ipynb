{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Mortality Prediction (DLMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results currently in this analysis \n",
    "\n",
    "- val loss for combined model\n",
    "- val loss for states only with combined data\n",
    "- val loss for countries only with combined data \n",
    "- val loss for combined data with Lee-Carter \n",
    "- missing important information in this file: loss for models with states and countries alone - is this in some other file? If not it maybe should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this analysis\n",
    " - assess performance of the deep learning model for US states and countries using a combined dataset\n",
    " - compare performance to baselines (Lee-Carter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible future questions worth asking \n",
    " - would it also improve relative to coherant LC extensions? \n",
    " - could do a more gradual increase in sample size (compare performance adding a tenth of the states at a time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PopName', 'Sex', 'Year', 'Age', 'mx']\n"
     ]
    }
   ],
   "source": [
    "# loading in USMDB data\n",
    "data = []\n",
    "ages = []\n",
    "states = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/usmdb/usmdb.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=',')\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            state, gender, year, age, rate = row\n",
    "            year = int(year)\n",
    "            try:\n",
    "                age = int(age)\n",
    "            except:\n",
    "                age = -1\n",
    "            if state not in states:\n",
    "                states.append(state)\n",
    "            state = states.index(state)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            # get rid of years, ages, not in health data and other cleaning\n",
    "            if age != -1 and rate != -1 and age <= 99:\n",
    "                data.append([state, gender, year, age, rate])\n",
    "\n",
    "state_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country', 'Gender', 'Year', 'Age', 'Mortality_rate']\n"
     ]
    }
   ],
   "source": [
    "# loading in HMD data\n",
    "data = []\n",
    "ages = []\n",
    "countries = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/hmd.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=\",\")\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            country, gender, year, age, rate = row\n",
    "            year = int(year)\n",
    "            try:\n",
    "                age = int(age)\n",
    "            except:\n",
    "                age = -1\n",
    "            if country not in countries:\n",
    "                countries.append(country)\n",
    "            country = countries.index(country)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            if age != -1 and rate != -1 and age <= 99:\n",
    "                data.append([country, gender, year, age, rate])\n",
    "\n",
    "country_data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Seperate DL Models for Country and State Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test sets \n",
    "training_index = np.logical_and(state_data[:, 2] >= 1959, state_data[:, 2] <= 2005)\n",
    "state_training = state_data[training_index, :]\n",
    "\n",
    "test_index = np.logical_and(state_data[:, 2] > 2005, state_data[:, 2] <= 2015)\n",
    "state_test = state_data[test_index, :]\n",
    "\n",
    "final_test_index = np.logical_and(state_data[:, 2] > 2015, state_data[:, 2] <= 2019)\n",
    "state_final_test = state_data[final_test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_training = tf.convert_to_tensor(state_training)\n",
    "state_test = tf.convert_to_tensor(state_test)\n",
    "state_final_test = tf.convert_to_tensor(state_final_test)\n",
    "# cast tensor to type float32\n",
    "state_training = tf.cast(state_training, tf.float32)\n",
    "state_test = tf.cast(state_test, tf.float32)\n",
    "state_final_test = tf.cast(state_final_test, tf.float32)\n",
    "\n",
    "num_train = state_training.shape[0]\n",
    "num_test = state_test.shape[0]\n",
    "num_final = state_final_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from training or test data (used in all iterations below)\n",
    "def get_data(index, training_data, test_data, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_data function to set up training and test tensorflow datasets \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_train = dataset_train.repeat()\n",
    "dataset_train = dataset_train.map(lambda x: get_data(x, state_training, state_test, mode=\"train\"), num_parallel_calls=4)\n",
    "dataset_train = dataset_train.batch(256)\n",
    "state_dataset_train = dataset_train.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_test = dataset_test.repeat()\n",
    "dataset_test = dataset_test.map(lambda x: get_data(x, state_training, state_test, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(256)\n",
    "state_dataset_test = dataset_test.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test2 = tf.data.Dataset.from_tensor_slices(np.arange(68000))\n",
    "dataset_test2 = dataset_test2.map(lambda x: get_data(x, state_training, state_test, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_test2 = dataset_test2.batch(256)\n",
    "state_dataset_test2 = dataset_test2.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_final = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_final = dataset_final.repeat()\n",
    "dataset_final = dataset_final.map(lambda x: get_data(x, state_training, state_test, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_final = dataset_final.batch(256)\n",
    "state_dataset_final = dataset_final.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(geo_dim):\n",
    "    # defining inputs \n",
    "    year = tfkl.Input(shape=(1,), dtype='float32', name='Year')\n",
    "    age =  tfkl.Input(shape=(1,), dtype='int32', name='Age')\n",
    "    geography = tfkl.Input(shape=(1,), dtype='int32', name='Geography')\n",
    "    gender = tfkl.Input(shape=(1,), dtype='int32', name='Gender')\n",
    "\n",
    "    # defining embedding layers \n",
    "    age_embed = tfkl.Embedding(input_dim=100, output_dim=5, name='Age_embed')(age)\n",
    "    age_embed = tfkl.Flatten()(age_embed)\n",
    "\n",
    "    gender_embed = tfkl.Embedding(input_dim=2, output_dim=5, name='Gender_embed')(gender)\n",
    "    gender_embed = tfkl.Flatten()(gender_embed)\n",
    "\n",
    "    geography_embed = tfkl.Embedding(input_dim=geo_dim, output_dim=5, name='Geography_embed')(geography)\n",
    "    geography_embed = tfkl.Flatten()(geography_embed)\n",
    "\n",
    "    # create feature vector that concatenates all inputs \n",
    "    x = tfkl.Concatenate()([year, age_embed, gender_embed, geography_embed])\n",
    "    x1 = x\n",
    "\n",
    "    # setting up middle layers \n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    # setting up output layer \n",
    "    x = tfkl.Concatenate()([x1, x])\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "    x = tfkl.Dense(1, activation='sigmoid', name='final')(x)\n",
    "\n",
    "    # creating the model \n",
    "    model = tf.keras.Model(inputs=[year, age, geography, gender], outputs=[x])\n",
    "\n",
    "    # compiling the model\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_model(dataset_train, dataset_test, geo_dim):\n",
    "    \n",
    "    model = create_model(geo_dim)\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, patience=3, verbose=0, mode=\"auto\", \n",
    "                                                    min_delta=1e-8, cooldown=0, min_lr=0.0)]\n",
    "    history = model.fit(dataset_train, steps_per_epoch=1000, validation_data=dataset_test, validation_steps=500, \n",
    "                        epochs=30, verbose=2, callbacks=callbacks)\n",
    "\n",
    "    loss_info = {\n",
    "        'train_mse': history.history['loss'][-1],\n",
    "        'val_mse': history.history['val_loss'][-1]\n",
    "    }\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    return model, loss_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# get the proper geography input dimension for model set up \n",
    "unique_vals = tf.unique(state_training[:, 0]).y\n",
    "state_geo_dim = np.array(tf.size(unique_vals)).item()\n",
    "print(geo_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 - 22s - 22ms/step - loss: 0.0140 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "1000/1000 - 16s - 16ms/step - loss: 6.5527e-04 - val_loss: 8.3409e-05 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "1000/1000 - 13s - 13ms/step - loss: 3.4390e-04 - val_loss: 5.4184e-04 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.4983e-04 - val_loss: 4.6172e-04 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "1000/1000 - 17s - 17ms/step - loss: 1.9296e-04 - val_loss: 7.4378e-05 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 1.6219e-04 - val_loss: 1.0699e-04 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 1.3653e-04 - val_loss: 4.3395e-05 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 1.1832e-04 - val_loss: 2.0244e-04 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 1.0922e-04 - val_loss: 6.2791e-05 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "1000/1000 - 16s - 16ms/step - loss: 1.0173e-04 - val_loss: 8.6651e-05 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "1000/1000 - 16s - 16ms/step - loss: 7.8250e-05 - val_loss: 4.0289e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 12/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 7.3137e-05 - val_loss: 4.2231e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 13/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 7.1086e-05 - val_loss: 3.8625e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 14/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 6.7959e-05 - val_loss: 4.1500e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 15/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 6.7630e-05 - val_loss: 5.3988e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 16/30\n",
      "1000/1000 - 18s - 18ms/step - loss: 6.5750e-05 - val_loss: 4.3318e-05 - learning_rate: 2.5000e-04\n",
      "Epoch 17/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 6.0890e-05 - val_loss: 5.8509e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 18/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 5.9946e-05 - val_loss: 5.2140e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 19/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 5.9343e-05 - val_loss: 5.4285e-05 - learning_rate: 6.2500e-05\n",
      "Epoch 20/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 5.9429e-05 - val_loss: 5.0894e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 21/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 5.8941e-05 - val_loss: 4.5918e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 22/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 5.9463e-05 - val_loss: 4.6710e-05 - learning_rate: 1.5625e-05\n",
      "Epoch 23/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 5.7732e-05 - val_loss: 4.4365e-05 - learning_rate: 3.9063e-06\n",
      "Epoch 24/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 5.9220e-05 - val_loss: 4.6479e-05 - learning_rate: 3.9063e-06\n",
      "Epoch 25/30\n",
      "1000/1000 - 13s - 13ms/step - loss: 5.9096e-05 - val_loss: 4.6812e-05 - learning_rate: 3.9063e-06\n",
      "Epoch 26/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 5.7887e-05 - val_loss: 4.6240e-05 - learning_rate: 9.7656e-07\n",
      "Epoch 27/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 5.8184e-05 - val_loss: 4.6485e-05 - learning_rate: 9.7656e-07\n",
      "Epoch 28/30\n",
      "1000/1000 - 13s - 13ms/step - loss: 5.8550e-05 - val_loss: 4.5569e-05 - learning_rate: 9.7656e-07\n",
      "Epoch 29/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 5.8573e-05 - val_loss: 4.6608e-05 - learning_rate: 2.4414e-07\n",
      "Epoch 30/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 5.8528e-05 - val_loss: 4.5660e-05 - learning_rate: 2.4414e-07\n"
     ]
    }
   ],
   "source": [
    "model_state, loss_info_state = run_deep_model(state_dataset_train, state_dataset_test, state_geo_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_mse': 5.8527719374978915e-05, 'val_mse': 4.5659751776838675e-05}\n"
     ]
    }
   ],
   "source": [
    "print(loss_info_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340794, 5)\n"
     ]
    }
   ],
   "source": [
    "# training and test sets \n",
    "training_index = np.logical_and(country_data[:, 2] >= 1959, country_data[:, 2] <= 2005)\n",
    "country_training = country_data[training_index, :]\n",
    "print(country_training.shape)\n",
    "\n",
    "test_index = np.logical_and(country_data[:, 2] > 2005, country_data[:, 2] <= 2015)\n",
    "country_test = country_data[test_index, :]\n",
    "\n",
    "final_test_index = np.logical_and(country_data[:, 2] > 2015, country_data[:, 2] <= 2019)\n",
    "country_final_test = country_data[final_test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_training = tf.convert_to_tensor(country_training)\n",
    "country_test = tf.convert_to_tensor(country_test)\n",
    "country_final_test = tf.convert_to_tensor(country_final_test)\n",
    "# cast tensor to type float32\n",
    "country_training = tf.cast(country_training, tf.float32)\n",
    "country_test = tf.cast(country_test, tf.float32)\n",
    "country_final_test = tf.cast(country_final_test, tf.float32)\n",
    "\n",
    "num_train = country_training.shape[0]\n",
    "num_test = country_test.shape[0]\n",
    "num_final = country_final_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_data function to set up training and test tensorflow datasets \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_train = dataset_train.repeat()\n",
    "dataset_train = dataset_train.map(lambda x: get_data(x, country_training, country_test, mode=\"train\"), num_parallel_calls=4)\n",
    "dataset_train = dataset_train.batch(256)\n",
    "country_dataset_train = dataset_train.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_test = dataset_test.repeat()\n",
    "dataset_test = dataset_test.map(lambda x: get_data(x, country_training, country_test, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(256)\n",
    "country_dataset_test = dataset_test.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test2 = tf.data.Dataset.from_tensor_slices(np.arange(68000))\n",
    "dataset_test2 = dataset_test2.map(lambda x: get_data(x, country_training, country_test, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_test2 = dataset_test2.batch(256)\n",
    "country_dataset_test2 = dataset_test2.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_final = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_final = dataset_final.repeat()\n",
    "dataset_final = dataset_final.map(lambda x: get_data(x, country_training, country_test, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_final = dataset_final.batch(256)\n",
    "country_dataset_final = dataset_final.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# get the proper geography input dimension for model set up \n",
    "unique_vals = tf.unique(country_training[:, 0]).y\n",
    "country_geo_dim = np.array(tf.size(unique_vals)).item()\n",
    "print(geo_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_country, loss_info_country = run_deep_model(country_dataset_train, country_dataset_test, country_geo_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(country_loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Combined DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# dropping US\u001b[39;00m\n\u001b[1;32m      5\u001b[0m country_data \u001b[38;5;241m=\u001b[39m country_data[country_data[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m87\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m countries\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# merge data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((state_data, country_data))\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "# getting unique values for geographic location column \n",
    "country_data[:,0] = country_data[:,0] + 50\n",
    "\n",
    "# dropping US\n",
    "country_data = country_data[country_data[:,0] != 87]\n",
    "countries.remove('USA')\n",
    "\n",
    "# merge data\n",
    "combined = np.vstack((state_data, country_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801394, 5)\n",
      "(172000, 5)\n",
      "(66400, 5)\n"
     ]
    }
   ],
   "source": [
    "training_index = np.logical_and(combined[:, 2] >= 1959, combined[:, 2] <= 2005)\n",
    "combined_training = combined[training_index, :]\n",
    "print(combined_training.shape)\n",
    "\n",
    "test_index = np.logical_and(combined[:, 2] > 2005, combined[:, 2] <= 2015)\n",
    "combined_test = combined[test_index, :]\n",
    "print(combined_test.shape)\n",
    "\n",
    "final_test_index = np.logical_and(combined[:, 2] > 2015, combined[:, 2] <= 2019)\n",
    "combined_final_test = combined[final_test_index, :]\n",
    "print(combined_final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and test data \n",
    "combined_training = tf.convert_to_tensor(combined_training)\n",
    "combined_test = tf.convert_to_tensor(combined_test)\n",
    "combined_final_test = tf.convert_to_tensor(combined_final_test)\n",
    "\n",
    "combined_training = tf.cast(combined_training, tf.float32)\n",
    "combined_test = tf.cast(combined_test, tf.float32)\n",
    "combined_final_test = tf.cast(combined_final_test, tf.float32)\n",
    "\n",
    "num_train = combined_training.shape[0]\n",
    "num_test = combined_test.shape[0]\n",
    "num_final = combined_final_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_data function to set up training and test tensorflow datasets \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_train = dataset_train.repeat()\n",
    "dataset_train = dataset_train.map(lambda x: get_data(x, combined_training, combined_test, mode=\"train\"), num_parallel_calls=4)\n",
    "dataset_train = dataset_train.batch(256)\n",
    "combined_dataset_train = dataset_train.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_test = dataset_test.repeat()\n",
    "dataset_test = dataset_test.map(lambda x: get_data(x, combined_training, combined_test, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(256)\n",
    "combined_dataset_test = dataset_test.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test2 = tf.data.Dataset.from_tensor_slices(np.arange(68000))\n",
    "dataset_test2 = dataset_test2.map(lambda x: get_data(x, combined_training, combined_test, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_test2 = dataset_test2.batch(256)\n",
    "combined_dataset_test2 = dataset_test2.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_final = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_final = dataset_final.repeat()\n",
    "dataset_final = dataset_final.map(lambda x: get_data(x, combined_training, combined_test, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_final = dataset_final.batch(256)\n",
    "combined_dataset_final = dataset_final.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "# get the proper geography input dimension for model set up \n",
    "unique_vals = tf.unique(combined_training[:, 0]).y\n",
    "combined_geo_dim = np.array(tf.size(unique_vals)).item()\n",
    "print(geo_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 - 22s - 22ms/step - loss: 0.0142 - val_loss: 3.4241e-04 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 0.0012 - val_loss: 5.8397e-04 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 7.1917e-04 - val_loss: 6.5208e-04 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "1000/1000 - 12s - 12ms/step - loss: 5.6941e-04 - val_loss: 1.8711e-04 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 4.7100e-04 - val_loss: 2.0635e-04 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "1000/1000 - 16s - 16ms/step - loss: 4.3342e-04 - val_loss: 1.7389e-04 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 4.0262e-04 - val_loss: 2.2599e-04 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 3.8968e-04 - val_loss: 1.6464e-04 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 3.8240e-04 - val_loss: 1.5528e-04 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 3.7677e-04 - val_loss: 2.0770e-04 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 3.5929e-04 - val_loss: 1.7163e-04 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 3.5703e-04 - val_loss: 2.3683e-04 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 3.1672e-04 - val_loss: 1.6332e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 14/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 3.0742e-04 - val_loss: 1.4760e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 15/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.9977e-04 - val_loss: 1.3083e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 16/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 3.1930e-04 - val_loss: 1.4886e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 17/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 3.0240e-04 - val_loss: 1.3842e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 18/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 3.0997e-04 - val_loss: 1.4496e-04 - learning_rate: 2.5000e-04\n",
      "Epoch 19/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 3.0225e-04 - val_loss: 1.4774e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 20/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.8850e-04 - val_loss: 1.5250e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 21/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.9247e-04 - val_loss: 1.3127e-04 - learning_rate: 6.2500e-05\n",
      "Epoch 22/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 2.9638e-04 - val_loss: 1.4667e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 23/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.8625e-04 - val_loss: 1.3434e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 24/30\n",
      "1000/1000 - 16s - 16ms/step - loss: 2.6641e-04 - val_loss: 1.4150e-04 - learning_rate: 1.5625e-05\n",
      "Epoch 25/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.8692e-04 - val_loss: 1.3195e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 26/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.9759e-04 - val_loss: 1.4031e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 27/30\n",
      "1000/1000 - 15s - 15ms/step - loss: 2.9705e-04 - val_loss: 1.4342e-04 - learning_rate: 3.9063e-06\n",
      "Epoch 28/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 3.0348e-04 - val_loss: 1.3634e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 29/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.9380e-04 - val_loss: 1.4091e-04 - learning_rate: 9.7656e-07\n",
      "Epoch 30/30\n",
      "1000/1000 - 14s - 14ms/step - loss: 2.9898e-04 - val_loss: 1.2738e-04 - learning_rate: 9.7656e-07\n"
     ]
    }
   ],
   "source": [
    "model_combined, loss_info_combined = run_deep_model(combined_dataset_train, combined_dataset_test, combined_geo_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_mse': 0.0002989772183354944, 'val_mse': 0.0001273844827665016}\n"
     ]
    }
   ],
   "source": [
    "# print combined loss info (loss for states and countries using full dataset)\n",
    "print(loss_info_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE for states only from combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_data(index, training_data, test_data, mode):\n",
    "    if mode == \"train\":\n",
    "        # Randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([], minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # Selects specified index from test data \n",
    "        entry = test_data[index, :]\n",
    "    else:  # Assuming mode=\"test\" or any other value\n",
    "        # For any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([], minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "\n",
    "    # Normalization\n",
    "    year = (year - 1998) / 21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "\n",
    "    # Reshape as needed (batched input)\n",
    "    features = (tf.reshape(year, [1]), tf.reshape(age, [1]), tf.reshape(geography, [1]), tf.reshape(gender, [1]))\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return features, rate\n",
    "\n",
    "# Creating the test dataset without `repeat()` for prediction\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "\n",
    "# Properly use `map` to call `get_data`\n",
    "dataset_test = dataset_test.map(lambda x: get_data(x, state_training, state_test, mode=\"not_random\"), num_parallel_calls=4)\n",
    "\n",
    "# Batch the dataset for efficient predictions\n",
    "dataset_test = dataset_test.batch(256)\n",
    "\n",
    "# Prefetch to improve performance\n",
    "state_dataset_test = dataset_test.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Now, run the predictions\n",
    "predictions = model_combined.predict(state_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.473827e-05\n"
     ]
    }
   ],
   "source": [
    "# get the true values from the test dataset\n",
    "true_values = []\n",
    "for _, rate in state_dataset_test:\n",
    "    true_values.extend(rate.numpy())\n",
    "\n",
    "# convert true_values to a numpy array\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# convert predictions to a numpy array if not already\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# compute MSE using TensorFlow\n",
    "mse = np.mean((true_values - predictions)**2)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE for countries only from combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Creating the test dataset without `repeat()` for prediction\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "\n",
    "# Properly use `map` to call `get_data`\n",
    "dataset_test = dataset_test.map(lambda x: get_data(x, country_training, country_test, mode=\"not_random\"), num_parallel_calls=4)\n",
    "\n",
    "# Batch the dataset for efficient predictions\n",
    "dataset_test = dataset_test.batch(256)\n",
    "\n",
    "# Prefetch to improve performance\n",
    "country_dataset_test = dataset_test.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Now, run the predictions\n",
    "predictions = model_combined.predict(country_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0005840145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 17:02:06.121582: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Get the true values from the test dataset\n",
    "true_values = []\n",
    "for _, rate in country_dataset_test:\n",
    "    true_values.extend(rate.numpy())\n",
    "\n",
    "# Convert true_values to a numpy array\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# Convert predictions to a numpy array if not already\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Compute MSE using TensorFlow\n",
    "mse = tf.reduce_mean(tf.square(true_values - predictions)).numpy()\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Lee-Carter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
      " 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
      " 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86.]\n"
     ]
    }
   ],
   "source": [
    "# non-tensor train / test split (same years in training / test here as in method above)\n",
    "training_index = np.logical_and(combined[:, 2] >= 1959, combined[:, 2] <= 2005)\n",
    "training_data = combined[training_index, :]\n",
    "\n",
    "test_index = np.logical_and(combined[:, 2] > 2005, combined[:, 2] <= 2015)\n",
    "test_data = combined[test_index, :]\n",
    "\n",
    "final_test_index = np.logical_and(combined[:, 2] > 2015, combined[:, 2] <= 2019)\n",
    "final_test = combined[final_test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up lee-carter function\n",
    "\n",
    "def lee_carter(mx_matrix):\n",
    "    \"\"\"\n",
    "    Run the Lee-Carter model on age-specific mortality data.\n",
    "    \n",
    "    Args:\n",
    "        mx_matrix (numpy.ndarray): A 2D array of age-specific mortality rates. rows = age, columns = years\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the estimated parameters (ax, bx, kt) and the fitted mortality rates.\n",
    "    \"\"\"\n",
    "    mx_matrix[mx_matrix <= 0] = 1e-9\n",
    "\n",
    "    ax = np.mean(np.log(mx_matrix), axis=1)\n",
    "    ax = ax.reshape(-1, 1) # reshape ax into column vector\n",
    "    \n",
    "    centered_mx = np.log(mx_matrix) - ax\n",
    "    \n",
    "    # SVD\n",
    "    U, S, Vt = np.linalg.svd(centered_mx, full_matrices=False)\n",
    "\n",
    "    # extract right and left singular vectors (bx and kt)\n",
    "    bx = U[:, 0]\n",
    "    kt = Vt[0, :]\n",
    "    # print(kt)\n",
    "\n",
    "    # normalize bx and kt \n",
    "    bx = bx / np.sum(bx)\n",
    "    # print(np.mean(kt))\n",
    "    kt = kt - np.mean(kt)\n",
    "    # print(np.mean(kt))\n",
    "\n",
    "    # estimate fitted mortality \n",
    "    fitted_mort = np.exp(ax + np.outer(bx, kt))\n",
    "\n",
    "    return (ax, bx, kt), fitted_mort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up function to run multiple models on all years in training data\n",
    "\n",
    "def lee_carter_geo_gender(data):\n",
    "\n",
    "    geos = np.unique(data[:, 0])\n",
    "    genders = np.unique(data[:, 1])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for geo in geos:\n",
    "        for gender in genders:\n",
    "            mask = (data[:, 0] == geo) & (data[:, 1] == gender)\n",
    "            geo_gender_data = data[mask]\n",
    "\n",
    "            # extract ages and years\n",
    "            years = np.unique(geo_gender_data[:, 2])\n",
    "            ages = np.unique(geo_gender_data[:, 3])\n",
    "\n",
    "            m_x = np.zeros((len(ages), len(years)))\n",
    "\n",
    "            for i, age in enumerate(ages):\n",
    "                for j, year in enumerate(years):\n",
    "                    mask = (geo_gender_data[:, 3] == age) & (geo_gender_data[:, 2] == year)\n",
    "                    #m_x[i,j] = geo_gender_data[mask, 4]\n",
    "                    selected_data = geo_gender_data[mask, 4]\n",
    "\n",
    "                    # Ensure we handle different cases for selected_data\n",
    "                    if selected_data.size == 0:\n",
    "                        # No data available for this age and year\n",
    "                        m_x[i, j] = np.nan  # Assign NaN or some default value\n",
    "                    elif selected_data.size == 1:\n",
    "                        # Exactly one value, the expected case\n",
    "                        m_x[i, j] = selected_data[0]\n",
    "                    else:\n",
    "                        # More than one value, choose an aggregation method\n",
    "                        print(\"more than 1 value\")\n",
    "                        m_x[i, j] = np.mean(selected_data)  # Or use np.median, np.min, etc.\n",
    "\n",
    "            # Debugging\n",
    "             # Check for NaN or infinite values\n",
    "            if np.isnan(m_x).any() or np.isinf(m_x).any():\n",
    "                print(f\"Skipping Geo: {geo}, Gender: {gender} due to NaN or infinite values in m_x\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                params, fitted_mort = lee_carter(m_x)\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                print(f\"SVD did not converge for Geo: {geo}, Gender: {gender}. Error: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            params, fitted_mort = lee_carter(m_x)\n",
    "    \n",
    "            # Store the results for the current geo and gender\n",
    "            results[(geo, gender)] = {\n",
    "                'params': params,\n",
    "                'fitted_mortality': fitted_mort\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lee_carter_forecast(results, h, start_year, ages, drift=True):\n",
    "    \"\"\"\n",
    "    Perform the forecasting step of the Lee-Carter method using a random walk with drift.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing the estimated parameters (ax, bx, kt) for each state and gender combination.\n",
    "        h (int): The number of future periods to forecast.\n",
    "        start_year (int): The starting year of the forecast.\n",
    "        ages (numpy.ndarray): A 1D array of ages corresponding to the rows of the mortality matrix.\n",
    "        drift (bool, optional): Whether to include a drift term in the random walk. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A 2D array with 5 columns representing state, gender, year, age, and forecasted mortality rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    forecasts = []\n",
    "    \n",
    "    for geo, gender in results.keys():\n",
    "        ax, bx, kt = results[(geo, gender)]['params']\n",
    "        \n",
    "        # Estimate the drift term\n",
    "        if drift:\n",
    "            drift_term = (kt[-1] - kt[0]) / (len(kt) - 1)\n",
    "        else:\n",
    "            drift_term = 0\n",
    "        \n",
    "        # Forecast future kt values using a random walk with drift\n",
    "        kt_forecast = np.zeros(h)\n",
    "        kt_forecast[0] = kt[-1]\n",
    "        for i in range(1, h):\n",
    "            kt_forecast[i] = kt_forecast[i-1] + drift_term + np.random.normal(0, 1)\n",
    "        \n",
    "        # Forecast future mortality rates\n",
    "        ax_matrix = np.repeat(ax, h).reshape(-1, h)\n",
    "        bx_matrix = np.repeat(bx, h).reshape(-1, h)\n",
    "        kt_matrix = np.repeat(kt_forecast, len(ax)).reshape(h, -1).T\n",
    "        mortality_forecast = np.exp(ax_matrix + bx_matrix * kt_matrix)\n",
    "\n",
    "        # Create a 2D array with geo, gender, year, age, and forecasted mortality rate\n",
    "        for i in range(h):\n",
    "            year = start_year + i\n",
    "            for j, age in enumerate(ages):\n",
    "                forecasts.append([geo, gender, year, age, mortality_forecast[j, i]])\n",
    "\n",
    "    # Convert forecasts to a NumPy array\n",
    "    forecasts = np.array(forecasts)\n",
    "\n",
    "    # Sort the forecasts array based on the first four columns\n",
    "    sorted_indices = np.lexsort((forecasts[:, 3], forecasts[:, 2], forecasts[:, 1], forecasts[:, 0]))\n",
    "    forecasts = forecasts[sorted_indices]\n",
    "\n",
    "    \n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(forecasted_rates, actual_rates):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between the forecasted and actual mortality rates.\n",
    "    \n",
    "    Args:\n",
    "        forecasted_rates (numpy.ndarray): A 2D array with 5 columns representing state, gender, year, age, and forecasted mortality rate.\n",
    "        actual_rates (numpy.ndarray): A 2D array with 5 columns representing state, gender, year, age, and actual mortality rate.\n",
    "        \n",
    "    Returns:\n",
    "        float: The Mean Squared Error (MSE) between the forecasted and actual mortality rates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure both arrays are sorted by geo, gender, year, and age\n",
    "    forecasted_rates = forecasted_rates[np.lexsort((forecasted_rates[:, 3], forecasted_rates[:, 2], forecasted_rates[:, 1], forecasted_rates[:, 0]))]\n",
    "    actual_rates = actual_rates[np.lexsort((actual_rates[:, 3], actual_rates[:, 2], actual_rates[:, 1], actual_rates[:, 0]))]\n",
    "\n",
    "    # Find common geo/gender/year/age combinations between forecasted and actual rates\n",
    "    common_keys = set(map(tuple, forecasted_rates[:, :4])) & set(map(tuple, actual_rates[:, :4]))\n",
    "\n",
    "    # Filter both forecasted and actual rates based on common combinations\n",
    "    filtered_forecasted = np.array([row for row in forecasted_rates if tuple(row[:4]) in common_keys])\n",
    "    filtered_actual = np.array([row for row in actual_rates if tuple(row[:4]) in common_keys])\n",
    "\n",
    "    # Extract the forecasted and actual mortality rates\n",
    "    forecasted_values = filtered_forecasted[:, 4]\n",
    "    actual_values = filtered_actual[:, 4]\n",
    "    \n",
    "    # Calculate the overall MSE\n",
    "    overall_mse = np.mean((forecasted_values - actual_values) ** 2)\n",
    "\n",
    "    # Filter for states and countries\n",
    "    states_mask = np.isin(filtered_forecasted[:, 0], range(0, 50))\n",
    "    countries_mask = np.isin(filtered_forecasted[:, 0], range(51, 87))\n",
    "\n",
    "    # Calculate MSE for states\n",
    "    states_forecasted_values = filtered_forecasted[states_mask, 4].astype(float)\n",
    "    states_actual_values = filtered_actual[states_mask, 4].astype(float)\n",
    "    states_mse = np.mean((states_forecasted_values - states_actual_values) ** 2)\n",
    "\n",
    "    # Calculate MSE for countries\n",
    "    countries_forecasted_values = filtered_forecasted[countries_mask, 4].astype(float)\n",
    "    countries_actual_values = filtered_actual[countries_mask, 4].astype(float)\n",
    "    countries_mse = np.mean((countries_forecasted_values - countries_actual_values) ** 2)\n",
    "    \n",
    "    return {\n",
    "        'overall_mse': overall_mse,\n",
    "        'states_mse': states_mse,\n",
    "        'countries_mse': countries_mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lc_model(train_data, test_data):\n",
    "    lc_output = lee_carter_geo_gender(train_data)\n",
    "    predictions = lee_carter_forecast(lc_output, h=10, start_year=2006, ages=range(0, 100))\n",
    "    test_mse = calculate_mse(predictions, test_data)\n",
    "\n",
    "    return test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Geo: 74.0, Gender: 1.0 due to NaN or infinite values in m_x\n"
     ]
    }
   ],
   "source": [
    "lc_results = run_lc_model(train_data=training_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_mse': 0.323552848214168,\n",
       " 'states_mse': 0.00019604996981467955,\n",
       " 'countries_mse': 0.8015493675903409}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Table 1: Training and Test MSEs\n",
    "This table will document average MSEs (for states alone, countries alone, and total) over 5 training runs with each model (LC, deep learning seperate, deep learning joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(num_iterations):\n",
    "    results = []\n",
    "    for i in range(num_iterations):\n",
    "        lc = run_lc_model(train_data=training_data, test_data=test_data)\n",
    "        state_only = run_deep_model(dataset_train=state_dataset_train, dataset_test=state_dataset_test, geo_dim=state_geo_dim)\n",
    "        country_only = run_deep_model(dataset_train=country_dataset_train, dataset_test=country_dataset_test, geo_dim=country_geo_dim)\n",
    "        results.append((lc, state_only, country_only))\n",
    "        print(f\"Loop {i}: lc {results[i][0]} & state only {results[i][1]} & country only {results[i][2]}\")\n",
    "\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/wn6shwbs4_9gcwtsrgz8v5vc0000gn/T/ipykernel_41906/1271048323.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  m_x[i,j] = state_gender_data[mask, 4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m comparison_results \u001b[38;5;241m=\u001b[39m compare_models(num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36mcompare_models\u001b[0;34m(num_iterations)\u001b[0m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m----> 4\u001b[0m     lc_results \u001b[38;5;241m=\u001b[39m run_lc_model(train_data\u001b[38;5;241m=\u001b[39mtraining_data, test_data\u001b[38;5;241m=\u001b[39mtest_data)\n\u001b[1;32m      5\u001b[0m     deep_results \u001b[38;5;241m=\u001b[39m run_deep_model(dataset_train\u001b[38;5;241m=\u001b[39mdataset_train, dataset_test\u001b[38;5;241m=\u001b[39mdataset_test)\n\u001b[1;32m      6\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((deep_results, lc_results))\n",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m, in \u001b[0;36mrun_lc_model\u001b[0;34m(train_data, test_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_lc_model\u001b[39m(train_data, test_data):\n\u001b[0;32m----> 2\u001b[0m     lc_output \u001b[38;5;241m=\u001b[39m lee_carter_state_gender(train_data)\n\u001b[1;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m lee_carter_forecast(lc_output, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, start_year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2006\u001b[39m, ages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m      4\u001b[0m     test_mse \u001b[38;5;241m=\u001b[39m calculate_mse(predictions, test_data)\n",
      "Cell \u001b[0;32mIn[30], line 24\u001b[0m, in \u001b[0;36mlee_carter_state_gender\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(years):\n\u001b[1;32m     23\u001b[0m         mask \u001b[38;5;241m=\u001b[39m (state_gender_data[:, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m age) \u001b[38;5;241m&\u001b[39m (state_gender_data[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m year)\n\u001b[0;32m---> 24\u001b[0m         m_x[i,j] \u001b[38;5;241m=\u001b[39m state_gender_data[mask, \u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     26\u001b[0m params, fitted_mort \u001b[38;5;241m=\u001b[39m lee_carter(m_x)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Store the results for the current state and gender\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "comparison_results = compare_models(num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m comp_results_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(comparison_results)\n\u001b[1;32m      2\u001b[0m comp_results_np\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comparison_results' is not defined"
     ]
    }
   ],
   "source": [
    "comp_results_np = np.array(comparison_results)\n",
    "comp_results_np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
