{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Mortality Prediction (DLMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this analysis\n",
    " - assess performance of the deep learning model given US state data, HMD country data, and both datasets \n",
    " - compare performance to baselines (Lee-Carter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible future questions worth asking: \n",
    " - would it also improve relative to coherant LC extensions? \n",
    " - could do a more gradual increase in sample size (compare performance adding a tenth of the states at a time) \n",
    " - county level estimates (do this next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PopName', 'Sex', 'Year', 'Age', 'mx']\n"
     ]
    }
   ],
   "source": [
    "# loading in USMDB data\n",
    "data = []\n",
    "ages = []\n",
    "states = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/usmdb/usmdb.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=',')\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            state, gender, year, age, rate = row\n",
    "            year = int(year)\n",
    "            try:\n",
    "                age = int(age)\n",
    "            except:\n",
    "                age = -1\n",
    "            if state not in states:\n",
    "                states.append(state)\n",
    "            state = states.index(state)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            # get rid of years, ages, not in health data and other cleaning\n",
    "            if age != -1 and rate != -1 and age <= 99:\n",
    "                data.append([state, gender, year, age, rate])\n",
    "\n",
    "state_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year', 'Age', 'Country', 'Gender', 'Mortality_rate']\n"
     ]
    }
   ],
   "source": [
    "# loading in HMD data\n",
    "data = []\n",
    "ages = []\n",
    "countries = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/Mx_1x1/hmd.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=\",\")\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            year, age, country, gender, rate = row\n",
    "            year = int(year)\n",
    "            try:\n",
    "                age = int(age)\n",
    "            except:\n",
    "                age = -1\n",
    "            if country not in countries:\n",
    "                countries.append(country)\n",
    "            country = countries.index(country)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            if age != -1 and rate != -1 and age <= 99:\n",
    "                data.append([country, gender, year, age, rate])\n",
    "\n",
    "country_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unique values for geographic location column \n",
    "country_data[:,0] = country_data[:,0] + 50\n",
    "\n",
    "# dropping US\n",
    "country_data = country_data[country_data[:,0] != 87]\n",
    "countries.remove('USA')\n",
    "\n",
    "# merge data\n",
    "combined = np.vstack((state_data, country_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create and prepare training and test sets \n",
    "def create_train_set(data, start_year, end_year) {\n",
    "    training_index = np.logical_and(data[:, 2] >= 1959, data[:, 2] <= 2005)\n",
    "    training_data = data[training_index, :]\n",
    "    \n",
    "    training_data = tf.convert_to_tensor(training_data)\n",
    "    training_data = tf.cast(training_data, tf.float32)\n",
    "    \n",
    "}\n",
    "\n",
    "create_test_set <- function(data, start_year, end_year) {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801394, 5)\n",
      "(172000, 5)\n",
      "(66400, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_index = np.logical_and(combined[:, 2] >= 1959, combined[:, 2] <= 2005)\n",
    "training_data = combined[training_index, :]\n",
    "print(training_data.shape)\n",
    "\n",
    "test_index = np.logical_and(combined[:, 2] > 2005, combined[:, 2] <= 2015)\n",
    "test_data = combined[test_index, :]\n",
    "print(test_data.shape)\n",
    "\n",
    "final_test_index = np.logical_and(combined[:, 2] > 2015, combined[:, 2] <= 2019)\n",
    "final_test = combined[final_test_index, :]\n",
    "print(final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and test data \n",
    "training_data = tf.convert_to_tensor(training_data)\n",
    "test_data = tf.convert_to_tensor(test_data)\n",
    "final_test = tf.convert_to_tensor(final_test)\n",
    "\n",
    "training_data = tf.cast(training_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)\n",
    "final_test = tf.cast(final_test, tf.float32)\n",
    "\n",
    "num_train = training_data.shape[0]\n",
    "num_test = test_data.shape[0]\n",
    "num_final = final_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from training or test data \n",
    "def get_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_data function to set up training and test tensorflow datasets \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_train = dataset_train.repeat()\n",
    "dataset_train = dataset_train.map(lambda x: get_data(x, mode=\"train\"), num_parallel_calls=4)\n",
    "dataset_train = dataset_train.batch(256)\n",
    "dataset_train = dataset_train.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_test = dataset_test.repeat()\n",
    "dataset_test = dataset_test.map(lambda x: get_data(x, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(256)\n",
    "dataset_test = dataset_test.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test2 = tf.data.Dataset.from_tensor_slices(np.arange(68000))\n",
    "dataset_test2 = dataset_test2.map(lambda x: get_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_test2 = dataset_test2.batch(256)\n",
    "dataset_test2 = dataset_test2.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_final = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_final = dataset_final.repeat()\n",
    "dataset_final = dataset_final.map(lambda x: get_data(x, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_final = dataset_final.batch(256)\n",
    "dataset_final = dataset_final.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # defining inputs \n",
    "    year = tfkl.Input(shape=(1,), dtype='float32', name='Year')\n",
    "    age =  tfkl.Input(shape=(1,), dtype='int32', name='Age')\n",
    "    geography = tfkl.Input(shape=(1,), dtype='int32', name='Geography')\n",
    "    gender = tfkl.Input(shape=(1,), dtype='int32', name='Gender')\n",
    "\n",
    "    # defining embedding layers \n",
    "    age_embed = tfkl.Embedding(input_dim=100, output_dim=5, input_length=1, name='Age_embed')(age)\n",
    "    age_embed = tfkl.Flatten()(age_embed)\n",
    "\n",
    "    gender_embed = tfkl.Embedding(input_dim=2, output_dim=5, input_length=1, name='Gender_embed')(gender)\n",
    "    gender_embed = tfkl.Flatten()(gender_embed)\n",
    "\n",
    "    geography_embed = tfkl.Embedding(input_dim=87, output_dim=5, input_length=1, name='Geography_embed')(geography)\n",
    "    geography_embed = tfkl.Flatten()(geography_embed)\n",
    "\n",
    "    # create feature vector that concatenates all inputs \n",
    "    x = tfkl.Concatenate()([year, age_embed, gender_embed, geography_embed])\n",
    "    x1 = x\n",
    "\n",
    "    # setting up middle layers \n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    # setting up output layer \n",
    "    x = tfkl.Concatenate()([x1, x])\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "    x = tfkl.Dense(1, activation='sigmoid', name='final')(x)\n",
    "\n",
    "    # creating the model \n",
    "    model = tf.keras.Model(inputs=[year, age, geography, gender], outputs=[x])\n",
    "\n",
    "    # compiling the model\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_model(dataset_train, dataset_test):\n",
    "    \n",
    "    model = create_model()\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, patience=3, verbose=0, mode=\"auto\", \n",
    "                                                    min_delta=1e-8, cooldown=0, min_lr=0.0)]\n",
    "    history = model.fit(dataset_train, steps_per_epoch=1000, validation_data=dataset_test, validation_steps=500, \n",
    "                        epochs=30, verbose=2, callbacks=callbacks)\n",
    "\n",
    "    loss_info = {\n",
    "        'train_mse': history.history['loss'][-1],\n",
    "        'val_mse': history.history['val_loss'][-1]\n",
    "    }\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    return model, loss_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 - 11s - loss: 0.0143 - val_loss: 0.0022 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 8s - loss: 0.0011 - val_loss: 6.9027e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 8s - loss: 7.6880e-04 - val_loss: 3.1531e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 8s - loss: 5.8665e-04 - val_loss: 5.0100e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 8s - loss: 4.8229e-04 - val_loss: 2.0311e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 8s - loss: 4.4073e-04 - val_loss: 2.0761e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 9s - loss: 4.2549e-04 - val_loss: 1.9609e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 8s - loss: 3.6890e-04 - val_loss: 2.6019e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 9s - loss: 3.5650e-04 - val_loss: 1.7954e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 9s - loss: 3.4732e-04 - val_loss: 2.3370e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 9s - loss: 3.5208e-04 - val_loss: 2.2604e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 9s - loss: 3.5598e-04 - val_loss: 1.5383e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 9s - loss: 3.4884e-04 - val_loss: 1.5681e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 9s - loss: 3.4286e-04 - val_loss: 2.0053e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 9s - loss: 3.3400e-04 - val_loss: 1.5468e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 9s - loss: 3.2315e-04 - val_loss: 1.4383e-04 - lr: 2.5000e-04 - 9s/epoch - 9ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 9s - loss: 3.1052e-04 - val_loss: 1.6210e-04 - lr: 2.5000e-04 - 9s/epoch - 9ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 9s - loss: 2.8664e-04 - val_loss: 1.4354e-04 - lr: 2.5000e-04 - 9s/epoch - 9ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 10s - loss: 3.0107e-04 - val_loss: 1.2799e-04 - lr: 2.5000e-04 - 10s/epoch - 10ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 9s - loss: 3.0538e-04 - val_loss: 1.3641e-04 - lr: 2.5000e-04 - 9s/epoch - 9ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 10s - loss: 3.0671e-04 - val_loss: 1.4330e-04 - lr: 2.5000e-04 - 10s/epoch - 10ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 10s - loss: 2.8108e-04 - val_loss: 1.5772e-04 - lr: 2.5000e-04 - 10s/epoch - 10ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 11s - loss: 2.8889e-04 - val_loss: 1.4288e-04 - lr: 6.2500e-05 - 11s/epoch - 11ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 10s - loss: 3.0515e-04 - val_loss: 1.2962e-04 - lr: 6.2500e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 10s - loss: 3.0819e-04 - val_loss: 1.5028e-04 - lr: 6.2500e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 10s - loss: 2.8666e-04 - val_loss: 1.2944e-04 - lr: 1.5625e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 11s - loss: 2.8448e-04 - val_loss: 1.2449e-04 - lr: 1.5625e-05 - 11s/epoch - 11ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 10s - loss: 2.8318e-04 - val_loss: 1.4852e-04 - lr: 1.5625e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 10s - loss: 2.8544e-04 - val_loss: 1.3549e-04 - lr: 1.5625e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 10s - loss: 2.9496e-04 - val_loss: 1.2915e-04 - lr: 1.5625e-05 - 10s/epoch - 10ms/step\n"
     ]
    }
   ],
   "source": [
    "model, loss_info = run_deep_model(dataset_train, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016744744789320976\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE for states only from combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n"
     ]
    }
   ],
   "source": [
    "# prep state test set \n",
    "state_test_index = np.logical_and(state_data[:, 2] > 2005, state_data[:, 2] <= 2015)\n",
    "state_test_data = state_data[state_test_index, :]\n",
    "state_test_data = tf.convert_to_tensor(state_test_data)\n",
    "state_test_data = tf.cast(state_test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from state training or test data \n",
    "def get_state_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = state_test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_state_test = tf.data.Dataset.from_tensor_slices(np.arange(100000))\n",
    "dataset_state_test = dataset_state_test.map(lambda x: get_state_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_state_test = dataset_state_test.batch(256)\n",
    "dataset_state_test = dataset_state_test.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# generate state predictions given model \n",
    "predictions = model.predict(dataset_state_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.246199e-05\n"
     ]
    }
   ],
   "source": [
    "# get the true values from the test dataset\n",
    "true_values = []\n",
    "for _, rate in dataset_state_test:\n",
    "    true_values.extend(rate.numpy())\n",
    "\n",
    "# convert true_values to a numpy array\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# convert predictions to a numpy array if not already\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# compute MSE using TensorFlow\n",
    "mse = np.mean((true_values - predictions)**2)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE for countries only from combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72000, 5)\n"
     ]
    }
   ],
   "source": [
    "# prep country test set \n",
    "country_test_index = np.logical_and(country_data[:, 2] > 2005, country_data[:, 2] <= 2015)\n",
    "country_test_data = country_data[country_test_index, :]\n",
    "country_test_data = tf.convert_to_tensor(country_test_data)\n",
    "country_test_data = tf.cast(country_test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from country training or test data \n",
    "def get_country_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = country_test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_country_test = tf.data.Dataset.from_tensor_slices(np.arange(72000))\n",
    "dataset_country_test = dataset_country_test.map(lambda x: get_country_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_country_test = dataset_country_test.batch(256)\n",
    "dataset_country_test = dataset_country_test.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 2s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(dataset_country_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0002767175\n"
     ]
    }
   ],
   "source": [
    "# Get the true values from the test dataset\n",
    "true_values = []\n",
    "for _, rate in dataset_country_test:\n",
    "    true_values.extend(rate.numpy())\n",
    "\n",
    "# Convert true_values to a numpy array\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# Convert predictions to a numpy array if not already\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Compute MSE using TensorFlow\n",
    "mse = tf.reduce_mean(tf.square(true_values - predictions)).numpy()\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Lee-Carter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(470000, 5)\n"
     ]
    }
   ],
   "source": [
    "# non-tensor train / test split (same years in training / test here as in method above)\n",
    "training_index = np.logical_and(data[:, 2] >= 1959, data[:, 2] <= 2005)\n",
    "training_data = data[training_index, :]\n",
    "print(training_data.shape)\n",
    "\n",
    "test_index = np.logical_and(data[:, 2] > 2005, data[:, 2] <= 2015)\n",
    "test_data = data[test_index, :]\n",
    "\n",
    "final_test_index = np.logical_and(data[:, 2] > 2015, data[:, 2] <= 2019)\n",
    "final_test = data[final_test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up lee-carter function\n",
    "\n",
    "def lee_carter(mx_matrix):\n",
    "    \"\"\"\n",
    "    Run the Lee-Carter model on age-specific mortality data.\n",
    "    \n",
    "    Args:\n",
    "        mx_matrix (numpy.ndarray): A 2D array of age-specific mortality rates. rows = age, columns = years\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the estimated parameters (ax, bx, kt) and the fitted mortality rates.\n",
    "    \"\"\"\n",
    "    mx_matrix[mx_matrix <= 0] = 1e-9\n",
    "\n",
    "    ax = np.mean(np.log(mx_matrix), axis=1)\n",
    "    ax = ax.reshape(-1, 1) # reshape ax into column vector\n",
    "    \n",
    "    centered_mx = np.log(mx_matrix) - ax\n",
    "    \n",
    "    # SVD\n",
    "    U, S, Vt = np.linalg.svd(centered_mx, full_matrices=False)\n",
    "\n",
    "    # extract right and left singular vectors (bx and kt)\n",
    "    bx = U[:, 0]\n",
    "    kt = Vt[0, :]\n",
    "    # print(kt)\n",
    "\n",
    "    # normalize bx and kt \n",
    "    bx = bx / np.sum(bx)\n",
    "    # print(np.mean(kt))\n",
    "    kt = kt - np.mean(kt)\n",
    "    # print(np.mean(kt))\n",
    "\n",
    "    # estimate fitted mortality \n",
    "    fitted_mort = np.exp(ax + np.outer(bx, kt))\n",
    "\n",
    "    return (ax, bx, kt), fitted_mort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up function to run multiple models on all years in training data\n",
    "\n",
    "def lee_carter_state_gender(data):\n",
    "\n",
    "    states = np.unique(data[:, 0])\n",
    "    genders = np.unique(data[:, 1])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for state in states:\n",
    "        for gender in genders:\n",
    "            mask = (data[:, 0] == state) & (data[:, 1] == gender)\n",
    "            state_gender_data = data[mask]\n",
    "\n",
    "            # extract ages and years\n",
    "            years = np.unique(state_gender_data[:, 2])\n",
    "            ages = np.unique(state_gender_data[:, 3])\n",
    "\n",
    "            m_x = np.zeros((len(ages), len(years)))\n",
    "\n",
    "            for i, age in enumerate(ages):\n",
    "                for j, year in enumerate(years):\n",
    "                    mask = (state_gender_data[:, 3] == age) & (state_gender_data[:, 2] == year)\n",
    "                    m_x[i,j] = state_gender_data[mask, 4]\n",
    "\n",
    "            params, fitted_mort = lee_carter(m_x)\n",
    "    \n",
    "            # Store the results for the current state and gender\n",
    "            results[(state, gender)] = {\n",
    "                'params': params,\n",
    "                'fitted_mortality': fitted_mort\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lee_carter_forecast(results, h, start_year, ages, drift=True):\n",
    "    \"\"\"\n",
    "    Perform the forecasting step of the Lee-Carter method using a random walk with drift.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing the estimated parameters (ax, bx, kt) for each state and gender combination.\n",
    "        h (int): The number of future periods to forecast.\n",
    "        start_year (int): The starting year of the forecast.\n",
    "        ages (numpy.ndarray): A 1D array of ages corresponding to the rows of the mortality matrix.\n",
    "        drift (bool, optional): Whether to include a drift term in the random walk. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A 2D array with 5 columns representing state, gender, year, age, and forecasted mortality rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    forecasts = []\n",
    "    \n",
    "    for state, gender in results.keys():\n",
    "        ax, bx, kt = results[(state, gender)]['params']\n",
    "        \n",
    "        # Estimate the drift term\n",
    "        if drift:\n",
    "            drift_term = (kt[-1] - kt[0]) / (len(kt) - 1)\n",
    "        else:\n",
    "            drift_term = 0\n",
    "        \n",
    "        # Forecast future kt values using a random walk with drift\n",
    "        kt_forecast = np.zeros(h)\n",
    "        kt_forecast[0] = kt[-1]\n",
    "        for i in range(1, h):\n",
    "            kt_forecast[i] = kt_forecast[i-1] + drift_term + np.random.normal(0, 1)\n",
    "        \n",
    "        # Forecast future mortality rates\n",
    "        ax_matrix = np.repeat(ax, h).reshape(-1, h)\n",
    "        bx_matrix = np.repeat(bx, h).reshape(-1, h)\n",
    "        kt_matrix = np.repeat(kt_forecast, len(ax)).reshape(h, -1).T\n",
    "        mortality_forecast = np.exp(ax_matrix + bx_matrix * kt_matrix)\n",
    "\n",
    "        # Create a 2D array with state, gender, year, age, and forecasted mortality rate\n",
    "        for i in range(h):\n",
    "            year = start_year + i\n",
    "            for j, age in enumerate(ages):\n",
    "                forecasts.append([state, gender, year, age, mortality_forecast[j, i]])\n",
    "\n",
    "    # Convert forecasts to a NumPy array\n",
    "    forecasts = np.array(forecasts)\n",
    "\n",
    "    # Sort the forecasts array based on the first four columns\n",
    "    sorted_indices = np.lexsort((forecasts[:, 3], forecasts[:, 2], forecasts[:, 1], forecasts[:, 0]))\n",
    "    forecasts = forecasts[sorted_indices]\n",
    "\n",
    "    \n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(forecasted_rates, actual_rates):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between the forecasted and actual mortality rates.\n",
    "    \n",
    "    Args:\n",
    "        forecasted_rates (numpy.ndarray): A 2D array with 5 columns representing state, gender, year, age, and forecasted mortality rate.\n",
    "        actual_rates (numpy.ndarray): A 2D array with 5 columns representing state, gender, year, age, and actual mortality rate.\n",
    "        \n",
    "    Returns:\n",
    "        float: The Mean Squared Error (MSE) between the forecasted and actual mortality rates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the forecasted and actual mortality rates\n",
    "    forecasted_values = forecasted_rates[:, 4]\n",
    "    actual_values = actual_rates[:, 4]\n",
    "    \n",
    "    # Calculate the squared differences between the forecasted and actual rates\n",
    "    squared_differences = (forecasted_values - actual_values) ** 2\n",
    "    \n",
    "    # Calculate the Mean Squared Error (MSE)\n",
    "    mse = np.mean(squared_differences)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lc_model(train_data, test_data):\n",
    "    lc_output = lee_carter_state_gender(train_data)\n",
    "    predictions = lee_carter_forecast(lc_output, h=10, start_year=2006, ages=range(0, 100))\n",
    "    test_mse = calculate_mse(predictions, test_data)\n",
    "\n",
    "    return np.array(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_results = run_lc_model(train_data=training_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Table 1: Training and Test MSEs\n",
    "This table will document average MSEs (for states alone, countries alone, and total) over 5 training runs with each model (LC, deep learning seperate, deep learning joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(num_iterations):\n",
    "    results = []\n",
    "    for i in range(num_iterations):\n",
    "        lc_results = run_lc_model(train_data=training_data, test_data=test_data)\n",
    "        deep_results = run_deep_model(dataset_train=dataset_train, dataset_test=dataset_test)\n",
    "        results.append((deep_results, lc_results))\n",
    "        print(f\"Loop {i}: deep mse {results[i][0]} & lc mse {results[i][1]}\")\n",
    "\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/wn6shwbs4_9gcwtsrgz8v5vc0000gn/T/ipykernel_88639/1271048323.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  m_x[i,j] = state_gender_data[mask, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 0: deep mse 4.0398939745500684e-05 & lc mse 0.0003915247299684933\n",
      "Loop 1: deep mse 6.0458645748440176e-05 & lc mse 0.00039128320488239905\n",
      "Loop 2: deep mse 4.817579247173853e-05 & lc mse 0.0003891320282911292\n",
      "Loop 3: deep mse 5.007610161555931e-05 & lc mse 0.0003862197077289283\n",
      "Loop 4: deep mse 4.09356398449745e-05 & lc mse 0.00038688559897930323\n",
      "Loop 5: deep mse 4.911636278848164e-05 & lc mse 0.0003864877662411712\n",
      "Loop 6: deep mse 4.301398075767793e-05 & lc mse 0.0003892479426403891\n",
      "Loop 7: deep mse 4.956285556545481e-05 & lc mse 0.00038826033560748026\n",
      "Loop 8: deep mse 4.017120591015555e-05 & lc mse 0.0003908170289735953\n",
      "Loop 9: deep mse 5.103383955429308e-05 & lc mse 0.00038729158184310007\n",
      "Loop 10: deep mse 4.477219408727251e-05 & lc mse 0.0003893728425534582\n",
      "Loop 11: deep mse 4.873637590208091e-05 & lc mse 0.00039045673061155\n",
      "Loop 12: deep mse 4.5513785153161734e-05 & lc mse 0.0003895944230450417\n",
      "Loop 13: deep mse 7.137803186196834e-05 & lc mse 0.00038838044018389434\n",
      "Loop 14: deep mse 5.818564386572689e-05 & lc mse 0.00038506183491556774\n",
      "Loop 15: deep mse 5.986831820337102e-05 & lc mse 0.0003870977087485554\n",
      "Loop 16: deep mse 4.108035864192061e-05 & lc mse 0.0003863712150338229\n",
      "Loop 17: deep mse 4.370341048343107e-05 & lc mse 0.00039019770114330634\n",
      "Loop 18: deep mse 4.055399767821655e-05 & lc mse 0.00039094099481334026\n",
      "Loop 19: deep mse 4.116302807233296e-05 & lc mse 0.00039022104129748535\n",
      "Loop 20: deep mse 4.791400351678021e-05 & lc mse 0.00038905815260852994\n",
      "Loop 21: deep mse 4.290840297471732e-05 & lc mse 0.00038547200411755784\n",
      "Loop 22: deep mse 6.165607192087919e-05 & lc mse 0.00038458043220354626\n",
      "Loop 23: deep mse 5.2702234825119376e-05 & lc mse 0.0003919180677142061\n",
      "Loop 24: deep mse 4.242498835083097e-05 & lc mse 0.00038749852383906686\n",
      "Loop 25: deep mse 4.239140616846271e-05 & lc mse 0.000386309489546373\n",
      "Loop 26: deep mse 6.20987921138294e-05 & lc mse 0.000388732004985983\n",
      "Loop 27: deep mse 4.934299431624822e-05 & lc mse 0.0003916216373170008\n",
      "Loop 28: deep mse 4.587364674080163e-05 & lc mse 0.000389226141050393\n",
      "Loop 29: deep mse 4.513711610343307e-05 & lc mse 0.00038740700920695306\n"
     ]
    }
   ],
   "source": [
    "comparison_results = compare_models(num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_results_np = np.array(comparison_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
