{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n",
    "tfkl = tf.keras.layers\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.layers import Input, Embedding, Flatten, Dense, BatchNormalization, Dropout, concatenate\n",
    "#from keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PopName', 'Sex', 'Year', 'Age', 'mx']\n"
     ]
    }
   ],
   "source": [
    "# loading in USMDB data\n",
    "data = []\n",
    "ages = []\n",
    "states = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/usmdb/usmdb.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=',')\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            state, gender, year, age, rate = row\n",
    "            year = int(year)\n",
    "            try:\n",
    "                age = int(age)\n",
    "            except:\n",
    "                age = -1\n",
    "            if state not in states:\n",
    "                states.append(state)\n",
    "            state = states.index(state)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            # get rid of years, ages, not in health data and other cleaning\n",
    "            if age != -1 and rate != -1 and age <= 99 and age >=18 and year >= 1993:\n",
    "                data.append([state, gender, year, age, rate])\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ... 49. 49. 49.]\n",
      "[[0.0000e+00 0.0000e+00 1.9930e+03 1.8000e+01 2.7000e-04]\n",
      " [0.0000e+00 0.0000e+00 1.9930e+03 1.9000e+01 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 1.9930e+03 2.0000e+01 1.0700e-03]\n",
      " ...\n",
      " [4.9000e+01 1.0000e+00 2.0200e+03 9.7000e+01 3.8917e-01]\n",
      " [4.9000e+01 1.0000e+00 2.0200e+03 9.8000e+01 4.2072e-01]\n",
      " [4.9000e+01 1.0000e+00 2.0200e+03 9.9000e+01 4.5292e-01]]\n",
      "['AK', 'AL', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
      "['f', 'm']\n"
     ]
    }
   ],
   "source": [
    "print(data[:,0])\n",
    "print(data)\n",
    "print(states)\n",
    "print(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/brfss/brfss_agg.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last minute cleaning. Will put chunk below into cleaning file soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      state    age     sex  year        bmi\n",
      "0        AL  18-24  female  1993  21.704250\n",
      "1        AL  18-24  female  1994  23.100608\n",
      "2        AL  18-24  female  1995  23.313510\n",
      "3        AL  18-24  female  1996  24.502406\n",
      "4        AL  18-24  female  1997  23.347746\n",
      "...     ...    ...     ...   ...        ...\n",
      "38849    WY    80+    male  2017  27.069832\n",
      "38850    WY    80+    male  2018  26.646026\n",
      "38851    WY    80+    male  2019  26.365848\n",
      "38852    WY    80+    male  2020  26.726692\n",
      "38853    WY    80+    male  2021  26.220133\n",
      "\n",
      "[37570 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# removing missing - should add this to cleaning dataset in the future\n",
    "index_missing = df[ (df['sex'] == '7')].index\n",
    "df.drop(index_missing, inplace=True)\n",
    "\n",
    "# Mapping from FIPS code to state abbreviation\n",
    "fips_to_abbreviation = {\n",
    "    1: 'AL', 2: 'AK', 4: 'AZ', 5: 'AR', 6: 'CA', 8: 'CO', 9: 'CT', 10: 'DE', 12: 'FL',\n",
    "    13: 'GA', 15: 'HI', 16: 'ID', 17: 'IL', 18: 'IN', 19: 'IA', 20: 'KS', 21: 'KY',\n",
    "    22: 'LA', 23: 'ME', 24: 'MD', 25: 'MA', 26: 'MI', 27: 'MN', 28: 'MS', 29: 'MO',\n",
    "    30: 'MT', 31: 'NE', 32: 'NV', 33: 'NH', 34: 'NJ', 35: 'NM', 36: 'NY', 37: 'NC',\n",
    "    38: 'ND', 39: 'OH', 40: 'OK', 41: 'OR', 42: 'PA', 44: 'RI', 45: 'SC', 46: 'SD',\n",
    "    47: 'TN', 48: 'TX', 49: 'UT', 50: 'VT', 51: 'VA', 53: 'WA', 54: 'WV', 55: 'WI',\n",
    "    56: 'WY'\n",
    "}\n",
    "\n",
    "# Convert FIPS codes to state abbreviations\n",
    "df['state'] = df['state'].map(fips_to_abbreviation)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state       int64\n",
      "age      category\n",
      "sex         int64\n",
      "year        int64\n",
      "bmi       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df['state'] = df['state'].astype('category')\n",
    "df['state'] = df['state'].apply(lambda x: states.index(x))\n",
    "df['state'] = df['state'].astype('int64')\n",
    "df['age'] = df['age'].astype('category')\n",
    "df['sex'] = df['sex'].map({'female': 0, 'male': 1})\n",
    "print(df.dtypes)\n",
    "\n",
    "health_data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train no covariate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training and test sets \n",
    "# training_index = np.logical_and(data[:, 2] >= 1998, data[:, 2] <= 2010)\n",
    "# training_data = data[training_index, :]\n",
    "# print(training_data.shape)\n",
    "\n",
    "# test_index = np.logical_and(data[:, 2] > 2010, data[:, 2] <= 2015)\n",
    "# test_data = data[test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = tf.convert_to_tensor(training_data)\n",
    "# test_data = tf.convert_to_tensor(test_data)\n",
    "# # cast tensor to type float32\n",
    "# training_data = tf.cast(training_data, tf.float32)\n",
    "# test_data = tf.cast(test_data, tf.float32)\n",
    "# num_train = training_data.shape[0]\n",
    "# num_test = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define function to fetch and process data entries from training or test data \n",
    "# def get_data(index, mode):\n",
    "#     if mode == \"train\":\n",
    "#         # randomly selects index from training data between 0 and num_train\n",
    "#         rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "#         entry = training_data[rand_index, :]\n",
    "#     elif mode == \"not_random\":\n",
    "#         # selects specified index from test data \n",
    "#         entry = test_data[index, :]\n",
    "#     else: \n",
    "#         # for any other value of mode, randomly selects index from test\n",
    "#         rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "#         entry = test_data[rand_index, :]\n",
    "#     state, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "#     year = (year - 1998)/21\n",
    "#     age = tf.cast(age, tf.int32)\n",
    "#     state = tf.cast(state, tf.int32)\n",
    "#     gender = tf.cast(gender, tf.int32)\n",
    "#     year = tf.reshape(year, [1])\n",
    "#     age = tf.reshape(age, [1])\n",
    "#     state = tf.reshape(state, [1])\n",
    "#     gender = tf.reshape(gender, [1])\n",
    "#     rate = tf.reshape(rate, [1])\n",
    "#     return (year, age, state, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_data function to set up training and test tensorflow datasets \n",
    "# dataset_train = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "# dataset_train = dataset_train.repeat()\n",
    "# dataset_train = dataset_train.map(lambda x: get_data(x, mode=\"train\"), num_parallel_calls=4)\n",
    "# dataset_train = dataset_train.batch(256)\n",
    "# dataset_train = dataset_train.prefetch(buffer_size=512)\n",
    "\n",
    "# dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "# dataset_test = dataset_test.repeat()\n",
    "# dataset_test = dataset_test.map(lambda x: get_data(x, mode=\"test\"), num_parallel_calls=4)\n",
    "# dataset_test = dataset_test.batch(256)\n",
    "# dataset_test = dataset_test.prefetch(buffer_size=512)\n",
    "\n",
    "# dataset_test2 = tf.data.Dataset.from_tensor_slices(np.arange(68000))\n",
    "# dataset_test2 = dataset_test2.map(lambda x: get_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "# dataset_test2 = dataset_test2.batch(256)\n",
    "# dataset_test2 = dataset_test2.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining inputs \n",
    "# year = tfkl.Input(shape=(1,), dtype='float32', name='Year')\n",
    "# age =  tfkl.Input(shape=(1,), dtype='int32', name='Age')\n",
    "# state = tfkl.Input(shape=(1,), dtype='int32', name='State')\n",
    "# gender = tfkl.Input(shape=(1,), dtype='int32', name='Gender')\n",
    "\n",
    "# # defining embedding layers \n",
    "# age_embed = tfkl.Embedding(input_dim=100, output_dim=5, input_length=1, name='Age_embed')(age)\n",
    "# age_embed = tfkl.Flatten()(age_embed)\n",
    "\n",
    "# gender_embed = tfkl.Embedding(input_dim=2, output_dim=5, input_length=1, name='Gender_embed')(gender)\n",
    "# gender_embed = tfkl.Flatten()(gender_embed)\n",
    "\n",
    "# state_embed = tfkl.Embedding(input_dim=50, output_dim=5, input_length=1, name='State_embed')(state)\n",
    "# state_embed = tfkl.Flatten()(state_embed)\n",
    "\n",
    "# # create feature vector that concatenates all inputs \n",
    "# x = tfkl.Concatenate()([year, age_embed, gender_embed, state_embed])\n",
    "# x1 = x\n",
    "\n",
    "# # setting up middle layers \n",
    "# x = tfkl.Dense(128, activation='tanh')(x)\n",
    "# x = tfkl.BatchNormalization()(x)\n",
    "# x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "# x = tfkl.Dense(128, activation='tanh')(x)\n",
    "# x = tfkl.BatchNormalization()(x)\n",
    "# x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "# x = tfkl.Dense(128, activation='tanh')(x)\n",
    "# x = tfkl.BatchNormalization()(x)\n",
    "# x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "# x = tfkl.Dense(128, activation='tanh')(x)\n",
    "# x = tfkl.BatchNormalization()(x)\n",
    "# x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "# # setting up output layer \n",
    "# x = tfkl.Concatenate()([x1, x])\n",
    "# x = tfkl.Dense(128, activation='tanh')(x)\n",
    "# x = tfkl.BatchNormalization()(x)\n",
    "# x = tfkl.Dropout(0.05)(x)\n",
    "# x = tfkl.Dense(1, activation='sigmoid', name='final')(x)\n",
    "\n",
    "# # creating the model \n",
    "# model = tf.keras.Model(inputs=[year, age, state, gender], outputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "# model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, patience=3, verbose=0, mode=\"auto\", min_delta=1e-8, cooldown=0, min_lr=0.0)]\n",
    "# model.fit(dataset_train, steps_per_epoch=1000, validation_data=dataset_test, validation_steps=500, epochs=30, verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train covariate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge health and mortality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to make state, age, gender, and year indexable\n",
    "\n",
    "age_groups = []\n",
    "states = []\n",
    "years = []\n",
    "shaped_data = np.empty((50, 13, 2, 29))\n",
    "shaped_data[:, :, :, :] = np.nan\n",
    "for row in health_data:\n",
    "    state = row[0]\n",
    "    age = row[1]\n",
    "    gender = row[2]\n",
    "    year = row[3]\n",
    "    bmi = row[4]\n",
    "\n",
    "    if not state in states:\n",
    "        states.append(state)\n",
    "    state = states.index(state)\n",
    "\n",
    "    if not age in age_groups:\n",
    "        age_groups.append(age)\n",
    "    age = age_groups.index(age)\n",
    "\n",
    "    if not year in years:\n",
    "        years.append(year)\n",
    "    year = years.index(year)\n",
    "\n",
    "    shaped_data[state, age, gender, year] = bmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using the mean of all states\n",
    "\n",
    "imputed_data = shaped_data.copy()\n",
    "matching = np.isnan(shaped_data)\n",
    "mean_values = np.repeat(np.nanmean(shaped_data, axis=0)[np.newaxis, :, :, :], 50, 0)\n",
    "imputed_data[matching] = mean_values[matching]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229600, 6)\n"
     ]
    }
   ],
   "source": [
    "combined_data = []\n",
    "for i, row in enumerate(data):\n",
    "    state = row[0]\n",
    "    gender = row[1]\n",
    "    year = row[2]\n",
    "    age = row[3]\n",
    "    rate = row[4]\n",
    "    \n",
    "    age_index = (age - 20) // 5\n",
    "    if age_index == -1:\n",
    "        age_index = 0\n",
    "    if age_index > 12:\n",
    "        age_index = 12\n",
    "\n",
    "    state_index = states.index(int(state))\n",
    "\n",
    "    year_index = years.index(year)\n",
    "\n",
    "    bmi = imputed_data[state_index, int(age_index), int(gender), year_index - 5]\n",
    "\n",
    "    combined_data.append([state, gender, year , age, bmi, rate])\n",
    "\n",
    "combined_data = np.array(combined_data)\n",
    "print(combined_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Covariate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the bmi variable\n",
    "combined_data[:, 4] = (combined_data[:, 4] - np.min(combined_data[:, 4])) / (np.max(combined_data[:, 4]) - np.min(combined_data[:, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106600, 6)\n"
     ]
    }
   ],
   "source": [
    "# training and test sets \n",
    "training_index = np.logical_and(combined_data[:, 2] >= 1998, combined_data[:, 2] <= 2010)\n",
    "training_data = combined_data[training_index, :]\n",
    "print(training_data.shape)\n",
    "\n",
    "test_index = np.logical_and(combined_data[:, 2] > 2010, combined_data[:, 2] <= 2015)\n",
    "test_data = combined_data[test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tf.convert_to_tensor(training_data)\n",
    "test_data = tf.convert_to_tensor(test_data)\n",
    "# cast tensor to type float32\n",
    "training_data = tf.cast(training_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)\n",
    "num_train = training_data.shape[0]\n",
    "num_test = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from training or test data \n",
    "def get_data(index, mode, include_age=True, include_gender=True, include_year=True, include_bmi=True, include_state=True):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    state, gender, year, age, bmi, rate = entry[0], entry[1], entry[2], entry[3], entry[4], entry[5]\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "\n",
    "     # Conditionally include bmi and state\n",
    "    inputs = []\n",
    "    if include_age:\n",
    "        age = tf.cast(age, tf.int32)\n",
    "        age = tf.reshape(age, [1])\n",
    "        inputs.append(age)\n",
    "    if include_gender:\n",
    "        gender = tf.cast(gender, tf.int32)\n",
    "        gender = tf.reshape(gender, [1])\n",
    "        inputs.append(gender)\n",
    "    if include_year:\n",
    "        year = (year - 1998)/21\n",
    "        year = tf.reshape(year, [1])\n",
    "        inputs.append(year)\n",
    "    if include_bmi:\n",
    "        bmi = tf.reshape(bmi, [1])\n",
    "        inputs.append(bmi)\n",
    "    if include_state:\n",
    "        state = tf.reshape(tf.cast(state, tf.int32), [1])\n",
    "        inputs.append(state)\n",
    "\n",
    "    return tuple(inputs), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_data function to set up training and test tensorflow datasets \n",
    "def create_dataset(mode, range, include_age=True, include_gender=True, include_year=True, include_bmi=True, include_state=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(np.arange(range))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.map(lambda x: get_data(x, mode=mode, include_age=include_age, include_gender=include_gender, include_year=include_year, include_bmi=include_bmi, include_state=include_state), \n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(256)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "dataset_test2 = create_dataset(mode=\"not_random\", range=68000)\n",
    "\n",
    "# dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "# dataset_test = dataset_test.repeat()\n",
    "# dataset_test = dataset_test.map(lambda x: get_data(x, mode=\"test\"), num_parallel_calls=4)\n",
    "# dataset_test = dataset_test.batch(256)\n",
    "# dataset_test = dataset_test.prefetch(buffer_size=512)\n",
    "\n",
    "# dataset_test2 = tf.data.Dataset.from_tensor_slices(np.arange(68000))\n",
    "# dataset_test2 = dataset_test2.map(lambda x: get_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "# dataset_test2 = dataset_test2.batch(256)\n",
    "# dataset_test2 = dataset_test2.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_model(include_age=True, include_gender=True, include_year=True, include_bmi=True, include_state=True):\n",
    "    # conditional inputs\n",
    "    if include_age:\n",
    "        age =  tfkl.Input(shape=(1,), dtype='int32', name='Age')\n",
    "    if include_gender:\n",
    "        gender = tfkl.Input(shape=(1,), dtype='int32', name='Gender')\n",
    "    if include_year:\n",
    "        year = tfkl.Input(shape=(1,), dtype='float32', name='Year')\n",
    "    if include_state:\n",
    "        state = tfkl.Input(shape=(1,), dtype='int32', name='State')\n",
    "    if include_bmi:\n",
    "        bmi = tfkl.Input(shape=(1,), dtype='float32', name='Bmi')\n",
    "\n",
    "    # defining embedding layers\n",
    "    if include_age:\n",
    "        age_embed = tfkl.Embedding(input_dim=100, output_dim=5, input_length=1, name='Age_embed')(age)\n",
    "        age_embed = tfkl.Flatten()(age_embed)\n",
    "\n",
    "    if include_gender:\n",
    "        gender_embed = tfkl.Embedding(input_dim=2, output_dim=5, input_length=1, name='Gender_embed')(gender)\n",
    "        gender_embed = tfkl.Flatten()(gender_embed)\n",
    "    \n",
    "    if include_state:\n",
    "        state_embed = tfkl.Embedding(input_dim=50, output_dim=5, input_length=1, name='State_embed')(state)\n",
    "        state_embed = tfkl.Flatten()(state_embed)\n",
    "\n",
    "    # create feature vector that concatenates all inputs \n",
    "    # x = tfkl.Concatenate()([year, age_embed, gender_embed, state_embed, bmi])\n",
    "    x = tfkl.Concatenate()(([age_embed] if include_age else []) + ([gender_embed] if include_gender else []) + ([year] if include_year else []) + ([state_embed] if include_state else []) + ([bmi] if include_bmi else []))\n",
    "    x1 = x\n",
    "\n",
    "    # setting up middle layers \n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    # setting up output layer \n",
    "    x = tfkl.Concatenate()([x1, x])\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "    outputs = tfkl.Dense(1, activation='sigmoid', name='final')(x)\n",
    "\n",
    "    # creating the model \n",
    "    inputs = []\n",
    "    if include_age:\n",
    "        inputs.append(age)\n",
    "    if include_gender:\n",
    "        inputs.append(gender)\n",
    "    if include_year:\n",
    "        inputs.append(year)\n",
    "    if include_state:\n",
    "        inputs.append(state)\n",
    "    if include_bmi:\n",
    "        inputs.append(bmi)\n",
    "    # model = tf.keras.Model(inputs=[year, age, state, gender, bmi], outputs=[x])\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # compiling the model\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss, val_loss):\n",
    "    loss = np.array(loss)\n",
    "    val_loss = np.array(val_loss)\n",
    "    loss = np.mean(loss)\n",
    "    val_loss = np.mean(val_loss)\n",
    "    return loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparitive_training(specs, epochs=30, runs=5):\n",
    "    results = []\n",
    "    for spec_name, include_age, include_gender, include_year, include_bmi, include_state in specs:\n",
    "        loss = []\n",
    "        val_loss = []\n",
    "        dataset_train = create_dataset(mode=\"train\", range=10000, include_age=include_age, include_gender=include_gender, include_year=include_year, include_bmi=include_bmi, include_state=include_state)\n",
    "        dataset_test = create_dataset(mode=\"test\", range=10000, include_age=include_age, include_gender=include_gender, include_year=include_year, include_bmi=include_bmi, include_state=include_state)\n",
    "        for i in range(runs):\n",
    "            model = redefine_model(include_age=include_age, include_gender=include_gender, include_year=include_year, include_bmi=include_bmi, include_state=include_state)\n",
    "            callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, patience=3, verbose=0, mode=\"auto\", min_delta=1e-8, cooldown=0, min_lr=0.0)]\n",
    "            history = model.fit(dataset_train, steps_per_epoch=1000, validation_data=dataset_test, validation_steps=500, epochs=epochs, verbose=2, callbacks=callbacks)\n",
    "            loss.append(history.history['loss'][-1])\n",
    "            val_loss.append(history.history['val_loss'][-1])\n",
    "        avg_loss, avg_val_loss = get_loss(loss, val_loss)\n",
    "        results.append((spec_name, avg_loss, avg_val_loss))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifications [(spec_name, include_age, include_gender, include_year, include_bmi, include_state)]\n",
    "specs = [\n",
    "    (\"No BMI\", True, True, True, False, True),\n",
    "    (\"With BMI\", True, True, True, True, True),\n",
    "    (\"No State or BMI\", True, True, True, False, False),\n",
    "    (\"BMI, no State\", True, True, True, True, False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = [\n",
    "    (\"Age\", True, False, False, False, False),\n",
    "    (\"Gender\", False, True, False, False, False),\n",
    "    (\"Year\", False, False, True, False, False),\n",
    "    (\"BMI\", False, False, False, True, False),\n",
    "    (\"State\", False, False, False, False, True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = []\n",
    "names = [\"Age\", \"Gender\", \"Year\", \"BMI\", \"State\"]\n",
    "for i, name in enumerate(names):\n",
    "    for j in range(len(names) - i - 1):\n",
    "        row = [name + \" \" + names[1 + i + j], False, False, False, False, False]\n",
    "        row[i+1] = True\n",
    "        row[2 + i + j] = True\n",
    "        specs.append((row[0], row[1], row[2], row[3], row[4], row[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 - 6s - loss: 0.0131 - val_loss: 3.1556e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 6s - loss: 0.0012 - val_loss: 3.0906e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 5s - loss: 7.4741e-04 - val_loss: 3.7330e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 5s - loss: 4.6563e-04 - val_loss: 2.6740e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 5s - loss: 3.0946e-04 - val_loss: 1.1684e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 6s - loss: 2.2660e-04 - val_loss: 1.6064e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 6s - loss: 1.6927e-04 - val_loss: 1.2743e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 6s - loss: 1.4275e-04 - val_loss: 1.7857e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 5s - loss: 1.1758e-04 - val_loss: 1.2486e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 5s - loss: 1.1619e-04 - val_loss: 8.7121e-05 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 5s - loss: 1.1169e-04 - val_loss: 1.5427e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 6s - loss: 1.0883e-04 - val_loss: 9.8756e-05 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 6s - loss: 1.0565e-04 - val_loss: 9.5000e-05 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 5s - loss: 1.0011e-04 - val_loss: 1.1058e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 5s - loss: 9.9740e-05 - val_loss: 9.3338e-05 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 5s - loss: 9.8433e-05 - val_loss: 1.0387e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 5s - loss: 9.7130e-05 - val_loss: 1.0204e-04 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 5s - loss: 9.7551e-05 - val_loss: 1.0395e-04 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 5s - loss: 9.8366e-05 - val_loss: 9.8079e-05 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 5s - loss: 9.6988e-05 - val_loss: 1.0543e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 5s - loss: 9.5998e-05 - val_loss: 1.0222e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 6s - loss: 9.4863e-05 - val_loss: 1.0124e-04 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 6s - loss: 9.6865e-05 - val_loss: 1.0356e-04 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 6s - loss: 9.5302e-05 - val_loss: 1.0189e-04 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 5s - loss: 9.5633e-05 - val_loss: 1.0221e-04 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 5s - loss: 9.5024e-05 - val_loss: 9.9384e-05 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 5s - loss: 9.6703e-05 - val_loss: 9.9017e-05 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 5s - loss: 9.6947e-05 - val_loss: 9.9931e-05 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 5s - loss: 9.5720e-05 - val_loss: 1.0143e-04 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 5s - loss: 9.5959e-05 - val_loss: 1.0210e-04 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 6s - loss: 0.0134 - val_loss: 0.0010 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 5s - loss: 9.9369e-04 - val_loss: 3.1628e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 6s - loss: 5.5537e-04 - val_loss: 2.3992e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 6s - loss: 4.1360e-04 - val_loss: 1.9781e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 6s - loss: 3.4969e-04 - val_loss: 2.2504e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 5s - loss: 3.1295e-04 - val_loss: 1.9676e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 6s - loss: 2.9236e-04 - val_loss: 2.7453e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 5s - loss: 2.7586e-04 - val_loss: 2.5546e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 6s - loss: 2.6475e-04 - val_loss: 1.7176e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 6s - loss: 2.5322e-04 - val_loss: 2.1331e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 5s - loss: 2.5671e-04 - val_loss: 1.7806e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 6s - loss: 2.4728e-04 - val_loss: 1.7877e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 6s - loss: 2.3278e-04 - val_loss: 1.7897e-04 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 6s - loss: 2.3417e-04 - val_loss: 1.7628e-04 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 5s - loss: 2.3096e-04 - val_loss: 1.8243e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 6s - loss: 2.2652e-04 - val_loss: 1.7514e-04 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 5s - loss: 2.2572e-04 - val_loss: 1.7815e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 6s - loss: 2.2415e-04 - val_loss: 1.7700e-04 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 6s - loss: 2.2361e-04 - val_loss: 1.7725e-04 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 6s - loss: 2.2464e-04 - val_loss: 1.7993e-04 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 5s - loss: 2.2369e-04 - val_loss: 1.7882e-04 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 5s - loss: 2.2230e-04 - val_loss: 1.7913e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 5s - loss: 2.2489e-04 - val_loss: 1.7429e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 5s - loss: 2.2539e-04 - val_loss: 1.7534e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 6s - loss: 2.2208e-04 - val_loss: 1.7944e-04 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 5s - loss: 2.2352e-04 - val_loss: 1.7740e-04 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 5s - loss: 2.2565e-04 - val_loss: 1.7668e-04 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 6s - loss: 2.2660e-04 - val_loss: 1.7748e-04 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 6s - loss: 2.2445e-04 - val_loss: 1.7608e-04 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 6s - loss: 2.2306e-04 - val_loss: 1.7720e-04 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 6s - loss: 0.0133 - val_loss: 3.5783e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 6s - loss: 0.0010 - val_loss: 8.1531e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 5s - loss: 6.4074e-04 - val_loss: 3.6223e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 5s - loss: 4.7564e-04 - val_loss: 5.8890e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 5s - loss: 3.6259e-04 - val_loss: 5.6500e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 5s - loss: 3.4430e-04 - val_loss: 2.3111e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 5s - loss: 3.2290e-04 - val_loss: 4.7837e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 5s - loss: 3.1424e-04 - val_loss: 5.5613e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 5s - loss: 3.0418e-04 - val_loss: 3.6189e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 5s - loss: 2.8257e-04 - val_loss: 2.5449e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 5s - loss: 2.7862e-04 - val_loss: 1.9824e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 5s - loss: 2.7719e-04 - val_loss: 2.3945e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 5s - loss: 2.7423e-04 - val_loss: 3.1593e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 5s - loss: 2.7083e-04 - val_loss: 2.4558e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 5s - loss: 2.6696e-04 - val_loss: 2.4272e-04 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 5s - loss: 2.6496e-04 - val_loss: 2.2552e-04 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 6s - loss: 2.6562e-04 - val_loss: 2.6525e-04 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 5s - loss: 2.6513e-04 - val_loss: 2.5911e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 6s - loss: 2.6493e-04 - val_loss: 2.7715e-04 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 6s - loss: 2.6103e-04 - val_loss: 2.6594e-04 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 5s - loss: 2.6525e-04 - val_loss: 2.6614e-04 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 5s - loss: 2.6396e-04 - val_loss: 2.7394e-04 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 5s - loss: 2.6360e-04 - val_loss: 2.5991e-04 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 5s - loss: 2.6488e-04 - val_loss: 2.6262e-04 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 5s - loss: 2.6384e-04 - val_loss: 2.6204e-04 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 5s - loss: 2.6335e-04 - val_loss: 2.5573e-04 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 5s - loss: 2.6108e-04 - val_loss: 2.6309e-04 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 6s - loss: 2.6258e-04 - val_loss: 2.5610e-04 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 5s - loss: 2.6301e-04 - val_loss: 2.5608e-04 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 5s - loss: 2.6479e-04 - val_loss: 2.6069e-04 - lr: 1.5259e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 6s - loss: 0.0134 - val_loss: 5.2751e-04 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 5s - loss: 0.0013 - val_loss: 5.4336e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 5s - loss: 9.0380e-04 - val_loss: 4.4881e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 5s - loss: 6.3340e-04 - val_loss: 4.5021e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 5s - loss: 4.8771e-04 - val_loss: 2.6555e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 5s - loss: 4.0142e-04 - val_loss: 2.9266e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 5s - loss: 3.2410e-04 - val_loss: 2.5362e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 5s - loss: 2.9027e-04 - val_loss: 2.3413e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 5s - loss: 2.6950e-04 - val_loss: 2.0050e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 5s - loss: 2.5540e-04 - val_loss: 1.8462e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 5s - loss: 2.4793e-04 - val_loss: 2.1260e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 5s - loss: 2.4454e-04 - val_loss: 1.9902e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 5s - loss: 2.4002e-04 - val_loss: 1.7748e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 5s - loss: 2.3522e-04 - val_loss: 2.1498e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 5s - loss: 2.3193e-04 - val_loss: 1.9864e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 5s - loss: 2.2936e-04 - val_loss: 2.1633e-04 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 5s - loss: 2.2214e-04 - val_loss: 1.8851e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 5s - loss: 2.2243e-04 - val_loss: 1.8654e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 5s - loss: 2.1993e-04 - val_loss: 1.9814e-04 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 5s - loss: 2.1803e-04 - val_loss: 1.8199e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 5s - loss: 2.1693e-04 - val_loss: 1.8305e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 5s - loss: 2.1805e-04 - val_loss: 1.8505e-04 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 5s - loss: 2.1686e-04 - val_loss: 1.9237e-04 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 5s - loss: 2.1577e-04 - val_loss: 1.8965e-04 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 5s - loss: 2.1535e-04 - val_loss: 1.8845e-04 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 5s - loss: 2.1594e-04 - val_loss: 1.8850e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 5s - loss: 2.1621e-04 - val_loss: 1.8982e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 5s - loss: 2.1483e-04 - val_loss: 1.8779e-04 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 5s - loss: 2.1674e-04 - val_loss: 1.9170e-04 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 5s - loss: 2.1771e-04 - val_loss: 1.8717e-04 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 5s - loss: 0.0205 - val_loss: 0.0084 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 5s - loss: 0.0094 - val_loss: 0.0082 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 5s - loss: 0.0095 - val_loss: 0.0083 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 5s - loss: 0.0093 - val_loss: 0.0081 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 5s - loss: 0.0093 - val_loss: 0.0082 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0082 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0082 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0083 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5000e-04 - 5s/epoch - 5ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 6s - loss: 0.0090 - val_loss: 0.0083 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0081 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0082 - lr: 1.5625e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.9063e-06 - 5s/epoch - 5ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0082 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0084 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.4414e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0082 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0083 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 5s - loss: 0.0091 - val_loss: 0.0082 - lr: 1.5259e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0082 - lr: 1.5259e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 6s - loss: 0.0186 - val_loss: 0.0068 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 6s - loss: 0.0076 - val_loss: 0.0066 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 6s - loss: 0.0073 - val_loss: 0.0070 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0070 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 6s - loss: 0.0071 - val_loss: 0.0068 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0067 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 6s - loss: 0.0070 - val_loss: 0.0066 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0065 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 6s - loss: 0.0070 - val_loss: 0.0068 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0064 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0064 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0067 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0070 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 6s - loss: 0.0068 - val_loss: 0.0068 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0067 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0065 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0067 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0067 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0065 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0067 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 6s - loss: 0.0068 - val_loss: 0.0067 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 6s - loss: 0.0069 - val_loss: 0.0066 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 7s - loss: 0.0205 - val_loss: 0.0085 - lr: 0.0010 - 7s/epoch - 7ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 6s - loss: 0.0096 - val_loss: 0.0082 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 6s - loss: 0.0095 - val_loss: 0.0082 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0083 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0083 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0083 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0082 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 6s - loss: 0.0090 - val_loss: 0.0082 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0081 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 6s - loss: 0.0194 - val_loss: 0.0067 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 6s - loss: 0.0078 - val_loss: 0.0069 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 6s - loss: 0.0077 - val_loss: 0.0060 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 6s - loss: 0.0075 - val_loss: 0.0057 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 6s - loss: 0.0074 - val_loss: 0.0057 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 6s - loss: 0.0073 - val_loss: 0.0057 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 6s - loss: 0.0073 - val_loss: 0.0058 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0075 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 6s - loss: 0.0073 - val_loss: 0.0059 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0058 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0058 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 6s - loss: 0.0071 - val_loss: 0.0059 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 6s - loss: 0.0071 - val_loss: 0.0059 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 6s - loss: 0.0071 - val_loss: 0.0059 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 6s - loss: 0.0071 - val_loss: 0.0059 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 6s - loss: 0.0071 - val_loss: 0.0060 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 6s - loss: 0.0071 - val_loss: 0.0059 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 6s - loss: 0.0071 - val_loss: 0.0058 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0058 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0059 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 6s - loss: 0.0072 - val_loss: 0.0060 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 6s - loss: 0.0203 - val_loss: 0.0083 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 5s - loss: 0.0096 - val_loss: 0.0082 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 6s - loss: 0.0095 - val_loss: 0.0082 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 6s - loss: 0.0094 - val_loss: 0.0083 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 6s - loss: 0.0094 - val_loss: 0.0082 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 5s - loss: 0.0093 - val_loss: 0.0083 - lr: 0.0010 - 5s/epoch - 5ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0083 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0083 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0082 - lr: 6.2500e-05 - 5s/epoch - 5ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0082 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0082 - lr: 9.7656e-07 - 5s/epoch - 5ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0084 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0083 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0083 - lr: 6.1035e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 1.5259e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 5s - loss: 0.0092 - val_loss: 0.0082 - lr: 1.5259e-08 - 5s/epoch - 5ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0083 - lr: 1.5259e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 1/30\n",
      "1000/1000 - 6s - loss: 0.0205 - val_loss: 0.0083 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 6s - loss: 0.0094 - val_loss: 0.0084 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0084 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 0.0010 - 6s/epoch - 6ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 2.5000e-04 - 6s/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 6.2500e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0083 - lr: 1.5625e-05 - 6s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0082 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 3.9063e-06 - 6s/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 9.7656e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 2.4414e-07 - 6s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0084 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 6.1035e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0082 - lr: 1.5259e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 6s - loss: 0.0092 - val_loss: 0.0083 - lr: 1.5259e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 6s - loss: 0.0093 - val_loss: 0.0084 - lr: 1.5259e-08 - 6s/epoch - 6ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 6s - loss: 0.0091 - val_loss: 0.0083 - lr: 3.8147e-09 - 6s/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "results = comparitive_training(specs, epochs=30, runs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Age Gender', 9.595869050826877e-05, 0.00010210151958744973), ('Age Year', 0.0002230644749943167, 0.000177202804479748), ('Age BMI', 0.00026478999643586576, 0.00026069296291098), ('Age State', 0.0002177057322114706, 0.00018716590420808643), ('Gender Year', 0.009157086722552776, 0.008234216831624508), ('Gender BMI', 0.006880640983581543, 0.006567842327058315), ('Gender State', 0.009100094437599182, 0.008172613568603992), ('Year BMI', 0.00717458501458168, 0.005954990163445473), ('Year State', 0.009277282282710075, 0.008251644670963287), ('BMI State', 0.009087569080293179, 0.00826759822666645)]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\centering\n",
      "\\begin{tabular}{lcc}\n",
      "\\hline\n",
      "Model Specification & Average Loss & Average Validation Loss \\\\ \\hline\n",
      "Age Gender & 0.000096 & 0.000102 \\\\\n",
      "Age Year & 0.000223 & 0.000177 \\\\\n",
      "Age BMI & 0.000265 & 0.000261 \\\\\n",
      "Age State & 0.000218 & 0.000187 \\\\\n",
      "Gender Year & 0.009157 & 0.008234 \\\\\n",
      "Gender BMI & 0.006881 & 0.006568 \\\\\n",
      "Gender State & 0.009100 & 0.008173 \\\\\n",
      "Year BMI & 0.007175 & 0.005955 \\\\\n",
      "Year State & 0.009277 & 0.008252 \\\\\n",
      "BMI State & 0.009088 & 0.008268 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Model Performance}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# Generate LaTeX table\n",
    "latex_table = \"\\\\begin{table}[H]\\n\\\\centering\\n\\\\begin{tabular}{lcc}\\n\\\\hline\\n\"\n",
    "latex_table += \"Model Specification & Average Loss & Average Validation Loss \\\\\\\\ \\\\hline\\n\"\n",
    "for spec_name, avg_loss, avg_val_loss in results:\n",
    "    latex_table += f\"{spec_name} & {avg_loss:.6f} & {avg_val_loss:.6f} \\\\\\\\\\n\"\n",
    "latex_table += \"\\\\hline\\n\\\\end{tabular}\\n\\\\caption{Model Performance}\\n\\\\end{table}\"\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BMI 1st run: loss: 6.5036e-05 - val_loss: 7.7524e-05 \n",
    "BMI 2nd run: loss: 7.0120e-05 - val_loss: 5.9770e-05\n",
    "BMI no lag: loss: 6.7715e-05 - val_loss: 5.2124e-05\n",
    "No State No BMI: loss: 6.5259e-05 - val_loss: 5.4376e-05\n",
    "State No BMI: loss: 3.5489e-05 - val_loss: 3.0721e-05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
