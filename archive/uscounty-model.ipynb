{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n",
    "tfkl = tf.keras.layers\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.layers import Input, Embedding, Flatten, Dense, BatchNormalization, Dropout, concatenate\n",
    "#from keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fips', 'year', 'age', 'sex', 'mx']\n"
     ]
    }
   ],
   "source": [
    "# loading in county data\n",
    "data = []\n",
    "ages = []\n",
    "counties = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/us_counties.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=',')\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            county, year, age, gender, rate = row\n",
    "            year = int(year)\n",
    "            if age not in ages:\n",
    "                ages.append(age)\n",
    "            age = ages.index(age)\n",
    "            if county not in counties:\n",
    "                counties.append(county)\n",
    "            county = counties.index(county)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            # get rid of years, ages, not in health data and other cleaning\n",
    "            if rate != -1:\n",
    "                data.append([county, gender, year, age, rate])\n",
    "\n",
    "county_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3569568, 5)\n"
     ]
    }
   ],
   "source": [
    "print(county_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PopName', 'Sex', 'Year', 'Age', 'mx']\n"
     ]
    }
   ],
   "source": [
    "# loading in USMDB data\n",
    "data = []\n",
    "ages = []\n",
    "states = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/usmdb_5x1.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=',')\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            state, gender, year, age, rate = row\n",
    "            year = int(year)\n",
    "            if age not in ages:\n",
    "                ages.append(age)\n",
    "            age = ages.index(age)\n",
    "            if state not in states:\n",
    "                states.append(state)\n",
    "            state = states.index(state)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            # get rid of years, ages, not in health data and other cleaning\n",
    "            if rate != -1:\n",
    "                data.append([state, gender, year, age, rate])\n",
    "\n",
    "state_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190272, 5)\n",
      "['AK', 'AL', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
      "['f', 'm']\n"
     ]
    }
   ],
   "source": [
    "print(state_data.shape)\n",
    "print(states)\n",
    "print(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year', 'Age', 'Country', 'Gender', 'Mortality_rate']\n"
     ]
    }
   ],
   "source": [
    "# loading in HMD data\n",
    "data = []\n",
    "ages = []\n",
    "countries = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/hmd_5yr.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=\",\")\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            year, age, country, gender, rate = row\n",
    "            year = int(year)\n",
    "            if age not in ages:\n",
    "                ages.append(age)\n",
    "            age = ages.index(age)\n",
    "            if country not in countries:\n",
    "                countries.append(country)\n",
    "            country = countries.index(country)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            if rate != -1:\n",
    "                data.append([country, gender, year, age, rate])\n",
    "\n",
    "country_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182736, 5)\n"
     ]
    }
   ],
   "source": [
    "print(country_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "print(len(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unique values for geographic location column \n",
    "country_data[:,0] = country_data[:,0] + 50\n",
    "county_data[:,0] = county_data[:,0] + 88\n",
    "\n",
    "# dropping US\n",
    "# country_data = country_data[country_data[:,0] != 87]\n",
    "# countries.remove('USA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(782697, 5)\n"
     ]
    }
   ],
   "source": [
    "print(country_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AUS', 'AUT', 'BEL', 'BGR', 'BLR', 'CAN', 'CHE', 'CZE', 'DNK', 'ESP', 'EST', 'FIN', 'FRATNP', 'GBRTENW', 'GBR_NIR', 'GBR_SCO', 'GRC', 'HUN', 'IRL', 'ISL', 'ISR', 'ITA', 'JPN', 'LTU', 'LUX', 'LVA', 'NLD', 'NOR', 'NZL_NM', 'POL', 'PRT', 'RUS', 'SVK', 'SVN', 'SWE', 'TWN', 'UKR', 'USA']\n"
     ]
    }
   ],
   "source": [
    "print(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.vstack((state_data, country_data, county_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3942576, 5)\n"
     ]
    }
   ],
   "source": [
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2445961, 5)\n",
      "(980656, 5)\n",
      "(391706, 5)\n"
     ]
    }
   ],
   "source": [
    "# training and test sets \n",
    "training_index = np.logical_and(combined[:, 2] >= 1959, combined[:, 2] <= 2005)\n",
    "training_data = combined[training_index, :]\n",
    "print(training_data.shape)\n",
    "\n",
    "test_index = np.logical_and(combined[:, 2] > 2005, combined[:, 2] <= 2015)\n",
    "test_data = combined[test_index, :]\n",
    "print(test_data.shape)\n",
    "\n",
    "final_test_index = np.logical_and(combined[:, 2] > 2015, combined[:, 2] <= 2019)\n",
    "final_test = combined[final_test_index, :]\n",
    "print(final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000e+00 0.00000e+00 2.00600e+03 0.00000e+00 8.35000e-03]\n",
      " [0.00000e+00 0.00000e+00 2.00600e+03 1.00000e+00 2.10000e-04]\n",
      " [0.00000e+00 0.00000e+00 2.00600e+03 2.00000e+00 6.30000e-04]\n",
      " ...\n",
      " [8.60000e+01 1.00000e+00 2.01300e+03 9.70000e+01 4.27027e-01]\n",
      " [8.60000e+01 1.00000e+00 2.01300e+03 9.80000e+01 4.76462e-01]\n",
      " [8.60000e+01 1.00000e+00 2.01300e+03 9.90000e+01 4.74544e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tf.convert_to_tensor(training_data)\n",
    "test_data = tf.convert_to_tensor(test_data)\n",
    "final_test = tf.convert_to_tensor(final_test)\n",
    "# cast tensor to type float32\n",
    "training_data = tf.cast(training_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)\n",
    "final_test = tf.cast(final_test, tf.float32)\n",
    "\n",
    "num_train = training_data.shape[0]\n",
    "num_test = test_data.shape[0]\n",
    "num_final = final_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.00000e+00 0.00000e+00 2.01600e+03 0.00000e+00 4.60000e-03]\n",
      " [0.00000e+00 0.00000e+00 2.01600e+03 1.00000e+00 7.40000e-04]\n",
      " [0.00000e+00 0.00000e+00 2.01600e+03 2.00000e+00 1.90000e-04]\n",
      " ...\n",
      " [8.50000e+01 1.00000e+00 2.01900e+03 9.70000e+01 2.77391e-01]\n",
      " [8.50000e+01 1.00000e+00 2.01900e+03 9.80000e+01 3.19445e-01]\n",
      " [8.50000e+01 1.00000e+00 2.01900e+03 9.90000e+01 3.32218e-01]], shape=(66400, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from training or test data \n",
    "def get_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_data function to set up training and test tensorflow datasets \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_train = dataset_train.repeat()\n",
    "dataset_train = dataset_train.map(lambda x: get_data(x, mode=\"train\"), num_parallel_calls=4)\n",
    "dataset_train = dataset_train.batch(256)\n",
    "dataset_train = dataset_train.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_test = dataset_test.repeat()\n",
    "dataset_test = dataset_test.map(lambda x: get_data(x, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(256)\n",
    "dataset_test = dataset_test.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test2 = tf.data.Dataset.from_tensor_slices(np.arange(68000))\n",
    "dataset_test2 = dataset_test2.map(lambda x: get_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_test2 = dataset_test2.batch(256)\n",
    "dataset_test2 = dataset_test2.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_final = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_final = dataset_final.repeat()\n",
    "dataset_final = dataset_final.map(lambda x: get_data(x, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_final = dataset_final.batch(256)\n",
    "dataset_final = dataset_final.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2472\n"
     ]
    }
   ],
   "source": [
    "print(len(counties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # defining inputs \n",
    "    year = tfkl.Input(shape=(1,), dtype='float32', name='Year')\n",
    "    age =  tfkl.Input(shape=(1,), dtype='int32', name='Age')\n",
    "    geography = tfkl.Input(shape=(1,), dtype='int32', name='Geography')\n",
    "    gender = tfkl.Input(shape=(1,), dtype='int32', name='Gender')\n",
    "\n",
    "    # defining embedding layers \n",
    "    age_embed = tfkl.Embedding(input_dim=24, output_dim=5, input_length=1, name='Age_embed')(age)\n",
    "    age_embed = tfkl.Flatten()(age_embed)\n",
    "\n",
    "    gender_embed = tfkl.Embedding(input_dim=2, output_dim=5, input_length=1, name='Gender_embed')(gender)\n",
    "    gender_embed = tfkl.Flatten()(gender_embed)\n",
    "\n",
    "    geography_embed = tfkl.Embedding(input_dim=2560, output_dim=5, input_length=1, name='Geography_embed')(geography)\n",
    "    geography_embed = tfkl.Flatten()(geography_embed)\n",
    "\n",
    "    # create feature vector that concatenates all inputs \n",
    "    x = tfkl.Concatenate()([year, age_embed, gender_embed, geography_embed])\n",
    "    x1 = x\n",
    "\n",
    "    # setting up middle layers \n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "\n",
    "    # setting up output layer \n",
    "    x = tfkl.Concatenate()([x1, x])\n",
    "    x = tfkl.Dense(128, activation='tanh')(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(0.05)(x)\n",
    "    x = tfkl.Dense(1, activation='sigmoid', name='final')(x)\n",
    "\n",
    "    # creating the model \n",
    "    model = tf.keras.Model(inputs=[year, age, geography, gender], outputs=[x])\n",
    "\n",
    "    # compiling the model\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_model(dataset_train, dataset_test):\n",
    "    \n",
    "    model = create_model()\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, patience=3, verbose=0, mode=\"auto\", \n",
    "                                                    min_delta=1e-8, cooldown=0, min_lr=0.0)]\n",
    "    history = model.fit(dataset_train, steps_per_epoch=1000, validation_data=dataset_test, validation_steps=500, \n",
    "                        epochs=30, verbose=2, callbacks=callbacks)\n",
    "\n",
    "    loss_info = history.history['val_loss'][-1]\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    return model, loss_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 - 19s - loss: 0.0150 - val_loss: 3.7353e-04 - lr: 0.0010 - 19s/epoch - 19ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 16s - loss: 9.5943e-04 - val_loss: 1.5268e-04 - lr: 0.0010 - 16s/epoch - 16ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 16s - loss: 8.0496e-04 - val_loss: 1.8149e-04 - lr: 0.0010 - 16s/epoch - 16ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 17s - loss: 6.2196e-04 - val_loss: 1.6144e-04 - lr: 0.0010 - 17s/epoch - 17ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 16s - loss: 4.9231e-04 - val_loss: 1.5566e-04 - lr: 0.0010 - 16s/epoch - 16ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 16s - loss: 3.5222e-04 - val_loss: 1.5863e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 17s - loss: 3.3397e-04 - val_loss: 1.8730e-04 - lr: 2.5000e-04 - 17s/epoch - 17ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 16s - loss: 3.5400e-04 - val_loss: 1.4268e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 16s - loss: 3.2209e-04 - val_loss: 1.2129e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 17s - loss: 3.4314e-04 - val_loss: 1.3444e-04 - lr: 2.5000e-04 - 17s/epoch - 17ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 16s - loss: 3.2267e-04 - val_loss: 1.5042e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 16s - loss: 3.0929e-04 - val_loss: 1.2112e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 16s - loss: 2.7464e-04 - val_loss: 1.1455e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 16s - loss: 3.0655e-04 - val_loss: 1.2350e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 16s - loss: 2.6188e-04 - val_loss: 1.0887e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 16s - loss: 2.4226e-04 - val_loss: 1.5511e-04 - lr: 2.5000e-04 - 16s/epoch - 16ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 18s - loss: 2.6559e-04 - val_loss: 1.8702e-04 - lr: 2.5000e-04 - 18s/epoch - 18ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 18s - loss: 2.6838e-04 - val_loss: 1.1226e-04 - lr: 2.5000e-04 - 18s/epoch - 18ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 16s - loss: 2.6724e-04 - val_loss: 1.5323e-04 - lr: 6.2500e-05 - 16s/epoch - 16ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 16s - loss: 2.6760e-04 - val_loss: 1.1053e-04 - lr: 6.2500e-05 - 16s/epoch - 16ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 16s - loss: 2.6891e-04 - val_loss: 1.1343e-04 - lr: 6.2500e-05 - 16s/epoch - 16ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 16s - loss: 2.4290e-04 - val_loss: 1.3917e-04 - lr: 1.5625e-05 - 16s/epoch - 16ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 16s - loss: 2.3745e-04 - val_loss: 1.3168e-04 - lr: 1.5625e-05 - 16s/epoch - 16ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 16s - loss: 2.6006e-04 - val_loss: 1.1107e-04 - lr: 1.5625e-05 - 16s/epoch - 16ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 16s - loss: 2.4392e-04 - val_loss: 1.3036e-04 - lr: 3.9063e-06 - 16s/epoch - 16ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 16s - loss: 2.4767e-04 - val_loss: 1.2268e-04 - lr: 3.9063e-06 - 16s/epoch - 16ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 16s - loss: 2.5483e-04 - val_loss: 1.5612e-04 - lr: 3.9063e-06 - 16s/epoch - 16ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 16s - loss: 2.3404e-04 - val_loss: 1.0447e-04 - lr: 9.7656e-07 - 16s/epoch - 16ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 16s - loss: 2.5228e-04 - val_loss: 1.1822e-04 - lr: 9.7656e-07 - 16s/epoch - 16ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 16s - loss: 2.7018e-04 - val_loss: 1.0782e-04 - lr: 9.7656e-07 - 16s/epoch - 16ms/step\n"
     ]
    }
   ],
   "source": [
    "model, loss_info = run_deep_model(dataset_train, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016744744789320976\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-Specific MSE from Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 5)\n"
     ]
    }
   ],
   "source": [
    "state_test_index = np.logical_and(state_data[:, 2] > 2005, state_data[:, 2] <= 2015)\n",
    "state_test_data = state_data[state_test_index, :]\n",
    "print(state_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_test_data = tf.convert_to_tensor(state_test_data)\n",
    "# cast tensor to type float32\n",
    "state_test_data = tf.cast(state_test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from training or test data \n",
    "def get_state_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = state_test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_state_test = tf.data.Dataset.from_tensor_slices(np.arange(7000))\n",
    "dataset_state_test = dataset_state_test.map(lambda x: get_state_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_state_test = dataset_state_test.batch(256)\n",
    "dataset_state_test = dataset_state_test.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(dataset_state_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0001929712\n"
     ]
    }
   ],
   "source": [
    "# Get the true values from the test dataset\n",
    "true_values = []\n",
    "for _, rate in dataset_state_test:\n",
    "    true_values.extend(rate.numpy())\n",
    "\n",
    "# Convert true_values to a numpy array\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# Convert predictions to a numpy array if not already\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Compute MSE using TensorFlow\n",
    "mse = np.mean((true_values - predictions)**2)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country-Specific MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72000, 5)\n"
     ]
    }
   ],
   "source": [
    "country_test_index = np.logical_and(country_data[:, 2] > 2005, country_data[:, 2] <= 2015)\n",
    "country_test_data = country_data[country_test_index, :]\n",
    "print(country_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_test_data = tf.convert_to_tensor(country_test_data)\n",
    "# cast tensor to type float32\n",
    "country_test_data = tf.cast(country_test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from training or test data \n",
    "def get_country_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = country_test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_country_test = tf.data.Dataset.from_tensor_slices(np.arange(72000))\n",
    "dataset_country_test = dataset_country_test.map(lambda x: get_country_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_country_test = dataset_country_test.batch(256)\n",
    "dataset_country_test = dataset_country_test.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 2s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(dataset_country_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0002767175\n"
     ]
    }
   ],
   "source": [
    "# Get the true values from the test dataset\n",
    "true_values = []\n",
    "for _, rate in dataset_country_test:\n",
    "    true_values.extend(rate.numpy())\n",
    "\n",
    "# Convert true_values to a numpy array\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# Convert predictions to a numpy array if not already\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Compute MSE using TensorFlow\n",
    "mse = tf.reduce_mean(tf.square(true_values - predictions)).numpy()\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
