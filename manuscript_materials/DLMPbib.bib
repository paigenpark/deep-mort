@article{andreagabrielliNeuralNetworkEmbedding2020,
  title = {Neural Network Embedding of the Over-Dispersed {{Poisson}} Reserving Model},
  author = {{Andrea Gabrielli} and Gabrielli, Andrea and {Ronald Richman} and Richman, Ronald and {Mario V. W{\"u}thrich} and W{\"u}thrich, Mario V.},
  year = {2020},
  month = jan,
  journal = {Scandinavian Actuarial Journal},
  volume = {2020},
  number = {1},
  pages = {1--29},
  doi = {10.1080/03461238.2019.1633394},
  abstract = {ABSTRACTThe main idea of this paper is to embed a classical actuarial regression model into a neural network architecture. This nesting allows us to learn model structure beyond the classical actua...},
  annotation = {MAG ID: 2902471886}
}

@article{francescaperlaTimeSeriesForecastingMortality2020,
  title = {Time-{{Series Forecasting}} of {{Mortality Rates}} Using {{Deep Learning}}},
  author = {{Francesca Perla} and Perla, Francesca and {Ronald Richman} and Richman, Ronald and {Salvatore Scognamiglio} and Scognamiglio, Salvatore and {Mario V. W{\"u}thrich} and W{\"u}thrich, Mario V.},
  year = {2020},
  journal = {Scandinavian Actuarial Journal},
  doi = {10.2139/ssrn.3595426},
  abstract = {The time-series nature of mortality rates lends itself to processing through neural networks that are specialized to deal with sequential data, such as recurrent and convolutional networks. Although appealing intuitively, a naive implementation of these networks does not lead to enhanced predictive performance. We show how the structure of the Lee Carter model can be generalized, and propose a relatively simple convolutional network model that can be interpreted as a generalization of the Lee Carter model, allowing for its components to be evaluated in familiar terms. The model produces highly accurate forecasts on the Human Mortality Database, and, without further modification, generalizes well to the United States Mortality Database.},
  annotation = {MAG ID: 3088920562\\
S2ID: d21727dee83a3ac4c1e52ecaa57809c1263138c3}
}

@book{francoischolletDeepLearningPython2017,
  title = {Deep {{Learning}} with {{Python}}},
  author = {{Fran{\c c}ois Chollet} and Chollet, Fran{\c c}ois},
  year = {2017},
  month = dec,
  abstract = {Summary Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learninga combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications. About the Book Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's Inside Deep learning from first principles Setting up your own deep-learning environment Image-classification models Deep learning for text and sequences Neural style transfer, text generation, and image generation About the Reader Readers need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required. About the Author Franois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others.},
  annotation = {MAG ID: 2784570262\\
S2ID: 3087b58cbfc6eb4a3076a180e21d6b872293f9a8}
}

@inproceedings{galDropout2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  pages = {1050--1059},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-09-30},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english},
  file = {/Users/paigepark/Zotero/storage/EX2PY623/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf}
}

@article{ronaldrichmanNeuralNetworkExtension2018,
  title = {A {{Neural Network Extension}} of the {{Lee-Carter Model}} to {{Multiple Populations}}},
  author = {{Ronald Richman} and Richman, Ronald and {Mario V. W{\"u}thrich} and W{\"u}thrich, Mario V.},
  year = {2018},
  month = oct,
  journal = {Annals of Actuarial Science},
  doi = {10.2139/ssrn.3270877},
  abstract = {The Lee-Carter model is a basic approach to forecasting mortality rates of a single population. Although extensions of the Lee-Carter model to forecasting rates for multiple populations have recently been proposed, the structure of these extended models is hard to justify and the models are often difficult to calibrate, relying on customized optimization schemes. Based on the paradigm of representation learning, we extend the Lee-Carter model to multiple populations using neural networks, which automatically select an optimal model structure. We fit this model to mortality rates since 1950 for all countries in the Human Mortality Database and observe that the out-of-sample forecasting performance of the model is highly competitive.},
  annotation = {MAG ID: 2900369797\\
S2ID: 684e22f0a442325dd3d6053842ba0389da7bc512}
}

@article{wangBayesian2016,
  title = {Towards {{Bayesian Deep Learning}}: {{A Framework}} and {{Some Existing Methods}}},
  shorttitle = {Towards {{Bayesian Deep Learning}}},
  author = {Wang, Hao and Yeung, Dit-Yan},
  year = {2016},
  month = dec,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {28},
  number = {12},
  pages = {3395--3408},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2016.2606428},
  urldate = {2024-09-30},
  abstract = {While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, subsequent tasks that involve inference, reasoning, and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as the Bayesian treatment of neural networks.},
  keywords = {Artificial intelligence,Bayes methods,Bayesian networks,data mining,deep learning,Learning systems,machine learning,Machine learning,Medical services,neural networks,Neural networks,Probabilistic logic,Recommender systems},
  file = {/Users/paigepark/Zotero/storage/QJSDK6XZ/Wang and Yeung - 2016 - Towards Bayesian Deep Learning A Framework and Some Existing Methods.pdf}
}
