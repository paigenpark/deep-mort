{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Mortality Prediction (DLMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this analysis\n",
    " - assess performance of the deep learning model given US state data, HMD country data, and both datasets \n",
    " - compare performance to baselines (Lee-Carter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible future questions worth asking: \n",
    " - would it also improve relative to coherant LC extensions? \n",
    " - could do a more gradual increase in sample size (compare performance adding a tenth of the states at a time) \n",
    " - county level estimates (do this next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (4.24.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorflow) (1.26.1)\n",
      "Requirement already satisfied: rich in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "tfkl = tf.keras.layers\n",
    "import keras\n",
    "from keras import ops\n",
    "kl = keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PopName', 'Sex', 'Year', 'Age', 'mx']\n"
     ]
    }
   ],
   "source": [
    "# loading in USMDB data\n",
    "data = []\n",
    "ages = []\n",
    "states = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/usmdb/usmdb.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=',')\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            state, gender, year, age, rate = row\n",
    "            year = int(year)\n",
    "            try:\n",
    "                age = int(age)\n",
    "            except:\n",
    "                age = -1\n",
    "            if state not in states:\n",
    "                states.append(state)\n",
    "            state = states.index(state)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            # get rid of years, ages, not in health data and other cleaning\n",
    "            if age != -1 and rate != -1 and age <= 99:\n",
    "                data.append([state, gender, year, age, rate])\n",
    "\n",
    "state_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Country', 'Gender', 'Year', 'Age', 'Mortality_rate']\n"
     ]
    }
   ],
   "source": [
    "# loading in HMD data\n",
    "data = []\n",
    "ages = []\n",
    "countries = []\n",
    "genders = []\n",
    "\n",
    "with open(\"../data/hmd.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file,delimiter=\",\")\n",
    "    for row_index, row in enumerate(reader):\n",
    "        if row_index == 0:\n",
    "            print(row)\n",
    "        if row_index >= 1:\n",
    "            country, gender, year, age, rate = row\n",
    "            year = int(year)\n",
    "            try:\n",
    "                age = int(age)\n",
    "            except:\n",
    "                age = -1\n",
    "            if country not in countries:\n",
    "                countries.append(country)\n",
    "            country = countries.index(country)\n",
    "            if gender not in genders:\n",
    "                genders.append(gender)\n",
    "            gender = genders.index(gender)\n",
    "            try:\n",
    "                rate = float(rate)\n",
    "            except:\n",
    "                rate = -1\n",
    "            if rate > 1:\n",
    "                rate = 1\n",
    "            if age != -1 and rate != -1 and age <= 99:\n",
    "                data.append([country, gender, year, age, rate])\n",
    "\n",
    "country_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting unique values for geographic location column \n",
    "country_data[:,0] = country_data[:,0] + 50\n",
    "\n",
    "# dropping US\n",
    "country_data = country_data[country_data[:,0] != 87]\n",
    "countries.remove('USA')\n",
    "\n",
    "# merge data\n",
    "combined = np.vstack((state_data, country_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801394, 5)\n",
      "(172000, 5)\n",
      "(66400, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_index = np.logical_and(combined[:, 2] >= 1959, combined[:, 2] <= 2005)\n",
    "training_data = combined[training_index, :]\n",
    "print(training_data.shape)\n",
    "\n",
    "test_index = np.logical_and(combined[:, 2] > 2005, combined[:, 2] <= 2015)\n",
    "test_data = combined[test_index, :]\n",
    "print(test_data.shape)\n",
    "\n",
    "final_test_index = np.logical_and(combined[:, 2] > 2015, combined[:, 2] <= 2019)\n",
    "final_test = combined[final_test_index, :]\n",
    "print(final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and test data \n",
    "training_data = tf.convert_to_tensor(training_data)\n",
    "test_data = tf.convert_to_tensor(test_data)\n",
    "final_test = tf.convert_to_tensor(final_test)\n",
    "\n",
    "training_data = tf.cast(training_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)\n",
    "final_test = tf.cast(final_test, tf.float32)\n",
    "\n",
    "num_train = training_data.shape[0]\n",
    "num_test = test_data.shape[0]\n",
    "num_final = final_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from training or test data \n",
    "def get_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_data function to set up training and test tensorflow datasets \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_train = dataset_train.repeat()\n",
    "dataset_train = dataset_train.map(lambda x: get_data(x, mode=\"train\"), num_parallel_calls=4)\n",
    "dataset_train = dataset_train.batch(256)\n",
    "dataset_train = dataset_train.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_test = dataset_test.repeat()\n",
    "dataset_test = dataset_test.map(lambda x: get_data(x, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_test = dataset_test.batch(256)\n",
    "dataset_test = dataset_test.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_test2 = tf.data.Dataset.from_tensor_slices(np.arange(68000))\n",
    "dataset_test2 = dataset_test2.map(lambda x: get_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_test2 = dataset_test2.batch(256)\n",
    "dataset_test2 = dataset_test2.prefetch(buffer_size=512)\n",
    "\n",
    "dataset_final = tf.data.Dataset.from_tensor_slices(np.arange(10000))\n",
    "dataset_final = dataset_final.repeat()\n",
    "dataset_final = dataset_final.map(lambda x: get_data(x, mode=\"test\"), num_parallel_calls=4)\n",
    "dataset_final = dataset_final.batch(256)\n",
    "dataset_final = dataset_final.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(kl.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = kl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [kl.Dense(ff_dim, activation=\"relu\"), kl.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = kl.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = kl.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = kl.Dropout(rate)\n",
    "        self.dropout2 = kl.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class DecoderBlock(kl.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att1 = kl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.att2 = kl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [kl.Dense(ff_dim, activation=\"relu\"), kl.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = kl.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = kl.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = kl.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = kl.Dropout(rate)\n",
    "        self.dropout2 = kl.Dropout(rate)\n",
    "        self.dropout3 = kl.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output):\n",
    "        attn1 = self.att1(x, x)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "        \n",
    "        attn2 = self.att2(out1, enc_output)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        return self.layernorm3(out2 + ffn_output)\n",
    "   \n",
    "class TokenEmbedding(kl.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = kl.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        return x\n",
    "\n",
    "def get_model():\n",
    "   \n",
    "    geo = kl.Input(shape=(1,))\n",
    "    gender = kl.Input(shape=(1,))\n",
    "    year = kl.Input(shape=(1,))\n",
    "    age = kl.Input(shape=(1,))\n",
    "    rate = kl.Input(shape=(1,))\n",
    "\n",
    "    emb_geo = TokenEmbedding(87, 32)(geo)\n",
    "    emb_gender = TokenEmbedding(2, 32)(gender)\n",
    "    emb_year = TokenEmbedding(60, 32)(year)\n",
    "    emb_age = TokenEmbedding(100, 32)(age)\n",
    "    # emb_rate = TokenEmbedding(30, 32)(rate)\n",
    "\n",
    "    x = kl.Concatenate(axis=-2)([emb_geo, emb_gender, emb_year, emb_age])\n",
    "   \n",
    "    for i in range(2):\n",
    "        x = TransformerBlock(32, 6, 128)(x)\n",
    "    \n",
    "    enc_output = x\n",
    "\n",
    "    # Decoder\n",
    "    emb_dec_input = TokenEmbedding(30, 10, 32)(rate)\n",
    "    x = emb_dec_input\n",
    "\n",
    "    for _ in range(2):\n",
    "        x = DecoderBlock(32, 6, 128)(x, enc_output)\n",
    "\n",
    "    outputs = kl.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(inputs=[numbers, actions], outputs=[outputs])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paigepark/anaconda3/envs/deep/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Age (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Gender (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Geography           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Age_embed           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span> │ Age[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Gender_embed        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> │ Gender[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Geography_embed     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">435</span> │ Geography[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Year (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Age_embed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Gender_embed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Geography_embed[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Year[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,560</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Age (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Gender (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Geography           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Age_embed           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │        \u001b[38;5;34m500\u001b[0m │ Age[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Gender_embed        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │         \u001b[38;5;34m10\u001b[0m │ Gender[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Geography_embed     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │        \u001b[38;5;34m435\u001b[0m │ Geography[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Year (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ Age_embed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ Gender_embed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ Geography_embed[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ Year[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,176\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m18,560\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">73,906</span> (288.70 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m73,906\u001b[0m (288.70 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,626</span> (283.70 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m72,626\u001b[0m (283.70 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> (5.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,280\u001b[0m (5.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_model(dataset_train, dataset_test):\n",
    "    \n",
    "    model = create_model()\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, patience=3, verbose=0, mode=\"auto\", \n",
    "                                                    min_delta=1e-8, cooldown=0, min_lr=0.0)]\n",
    "    history = model.fit(dataset_train, steps_per_epoch=1000, validation_data=dataset_test, validation_steps=500, \n",
    "                        epochs=30, verbose=2, callbacks=callbacks)\n",
    "\n",
    "    loss_info = {\n",
    "        'train_mse': history.history['loss'][-1],\n",
    "        'val_mse': history.history['val_loss'][-1]\n",
    "    }\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    return model, loss_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 - 11s - loss: 0.0143 - val_loss: 0.0022 - lr: 0.0010 - 11s/epoch - 11ms/step\n",
      "Epoch 2/30\n",
      "1000/1000 - 8s - loss: 0.0011 - val_loss: 6.9027e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 3/30\n",
      "1000/1000 - 8s - loss: 7.6880e-04 - val_loss: 3.1531e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 4/30\n",
      "1000/1000 - 8s - loss: 5.8665e-04 - val_loss: 5.0100e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 5/30\n",
      "1000/1000 - 8s - loss: 4.8229e-04 - val_loss: 2.0311e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 6/30\n",
      "1000/1000 - 8s - loss: 4.4073e-04 - val_loss: 2.0761e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 7/30\n",
      "1000/1000 - 9s - loss: 4.2549e-04 - val_loss: 1.9609e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 8/30\n",
      "1000/1000 - 8s - loss: 3.6890e-04 - val_loss: 2.6019e-04 - lr: 0.0010 - 8s/epoch - 8ms/step\n",
      "Epoch 9/30\n",
      "1000/1000 - 9s - loss: 3.5650e-04 - val_loss: 1.7954e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 10/30\n",
      "1000/1000 - 9s - loss: 3.4732e-04 - val_loss: 2.3370e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 11/30\n",
      "1000/1000 - 9s - loss: 3.5208e-04 - val_loss: 2.2604e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 12/30\n",
      "1000/1000 - 9s - loss: 3.5598e-04 - val_loss: 1.5383e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 13/30\n",
      "1000/1000 - 9s - loss: 3.4884e-04 - val_loss: 1.5681e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 14/30\n",
      "1000/1000 - 9s - loss: 3.4286e-04 - val_loss: 2.0053e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 15/30\n",
      "1000/1000 - 9s - loss: 3.3400e-04 - val_loss: 1.5468e-04 - lr: 0.0010 - 9s/epoch - 9ms/step\n",
      "Epoch 16/30\n",
      "1000/1000 - 9s - loss: 3.2315e-04 - val_loss: 1.4383e-04 - lr: 2.5000e-04 - 9s/epoch - 9ms/step\n",
      "Epoch 17/30\n",
      "1000/1000 - 9s - loss: 3.1052e-04 - val_loss: 1.6210e-04 - lr: 2.5000e-04 - 9s/epoch - 9ms/step\n",
      "Epoch 18/30\n",
      "1000/1000 - 9s - loss: 2.8664e-04 - val_loss: 1.4354e-04 - lr: 2.5000e-04 - 9s/epoch - 9ms/step\n",
      "Epoch 19/30\n",
      "1000/1000 - 10s - loss: 3.0107e-04 - val_loss: 1.2799e-04 - lr: 2.5000e-04 - 10s/epoch - 10ms/step\n",
      "Epoch 20/30\n",
      "1000/1000 - 9s - loss: 3.0538e-04 - val_loss: 1.3641e-04 - lr: 2.5000e-04 - 9s/epoch - 9ms/step\n",
      "Epoch 21/30\n",
      "1000/1000 - 10s - loss: 3.0671e-04 - val_loss: 1.4330e-04 - lr: 2.5000e-04 - 10s/epoch - 10ms/step\n",
      "Epoch 22/30\n",
      "1000/1000 - 10s - loss: 2.8108e-04 - val_loss: 1.5772e-04 - lr: 2.5000e-04 - 10s/epoch - 10ms/step\n",
      "Epoch 23/30\n",
      "1000/1000 - 11s - loss: 2.8889e-04 - val_loss: 1.4288e-04 - lr: 6.2500e-05 - 11s/epoch - 11ms/step\n",
      "Epoch 24/30\n",
      "1000/1000 - 10s - loss: 3.0515e-04 - val_loss: 1.2962e-04 - lr: 6.2500e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 25/30\n",
      "1000/1000 - 10s - loss: 3.0819e-04 - val_loss: 1.5028e-04 - lr: 6.2500e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 26/30\n",
      "1000/1000 - 10s - loss: 2.8666e-04 - val_loss: 1.2944e-04 - lr: 1.5625e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 27/30\n",
      "1000/1000 - 11s - loss: 2.8448e-04 - val_loss: 1.2449e-04 - lr: 1.5625e-05 - 11s/epoch - 11ms/step\n",
      "Epoch 28/30\n",
      "1000/1000 - 10s - loss: 2.8318e-04 - val_loss: 1.4852e-04 - lr: 1.5625e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 29/30\n",
      "1000/1000 - 10s - loss: 2.8544e-04 - val_loss: 1.3549e-04 - lr: 1.5625e-05 - 10s/epoch - 10ms/step\n",
      "Epoch 30/30\n",
      "1000/1000 - 10s - loss: 2.9496e-04 - val_loss: 1.2915e-04 - lr: 1.5625e-05 - 10s/epoch - 10ms/step\n"
     ]
    }
   ],
   "source": [
    "model, loss_info = run_deep_model(dataset_train, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016744744789320976\n"
     ]
    }
   ],
   "source": [
    "print(loss_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE for states only from combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n"
     ]
    }
   ],
   "source": [
    "# prep state test set \n",
    "state_test_index = np.logical_and(state_data[:, 2] > 2005, state_data[:, 2] <= 2015)\n",
    "state_test_data = state_data[state_test_index, :]\n",
    "state_test_data = tf.convert_to_tensor(state_test_data)\n",
    "state_test_data = tf.cast(state_test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from state training or test data \n",
    "def get_state_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = state_test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_state_test = tf.data.Dataset.from_tensor_slices(np.arange(100000))\n",
    "dataset_state_test = dataset_state_test.map(lambda x: get_state_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_state_test = dataset_state_test.batch(256)\n",
    "dataset_state_test = dataset_state_test.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# generate state predictions given model \n",
    "predictions = model.predict(dataset_state_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.246199e-05\n"
     ]
    }
   ],
   "source": [
    "# get the true values from the test dataset\n",
    "true_values = []\n",
    "for _, rate in dataset_state_test:\n",
    "    true_values.extend(rate.numpy())\n",
    "\n",
    "# convert true_values to a numpy array\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# convert predictions to a numpy array if not already\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# compute MSE using TensorFlow\n",
    "mse = np.mean((true_values - predictions)**2)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE for countries only from combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72000, 5)\n"
     ]
    }
   ],
   "source": [
    "# prep country test set \n",
    "country_test_index = np.logical_and(country_data[:, 2] > 2005, country_data[:, 2] <= 2015)\n",
    "country_test_data = country_data[country_test_index, :]\n",
    "country_test_data = tf.convert_to_tensor(country_test_data)\n",
    "country_test_data = tf.cast(country_test_data, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to fetch and process data entries from country training or test data \n",
    "def get_country_data(index, mode):\n",
    "    if mode == \"train\":\n",
    "        # randomly selects index from training data between 0 and num_train\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_train, dtype=tf.int32) \n",
    "        entry = training_data[rand_index, :]\n",
    "    elif mode == \"not_random\":\n",
    "        # selects specified index from test data \n",
    "        entry = country_test_data[index, :]\n",
    "    else: \n",
    "        # for any other value of mode, randomly selects index from test\n",
    "        rand_index = tf.random.uniform([],minval=0, maxval=num_test, dtype=tf.int32)\n",
    "        entry = test_data[rand_index, :]\n",
    "    geography, gender, year, age, rate = entry[0], entry[1], entry[2], entry[3], entry[4]\n",
    "    year = (year - 1998)/21\n",
    "    age = tf.cast(age, tf.int32)\n",
    "    geography = tf.cast(geography, tf.int32)\n",
    "    gender = tf.cast(gender, tf.int32)\n",
    "    year = tf.reshape(year, [1])\n",
    "    age = tf.reshape(age, [1])\n",
    "    geography = tf.reshape(geography, [1])\n",
    "    gender = tf.reshape(gender, [1])\n",
    "    rate = tf.reshape(rate, [1])\n",
    "    return (year, age, geography, gender), rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_country_test = tf.data.Dataset.from_tensor_slices(np.arange(72000))\n",
    "dataset_country_test = dataset_country_test.map(lambda x: get_country_data(x, mode=\"not_random\"), num_parallel_calls=4)\n",
    "dataset_country_test = dataset_country_test.batch(256)\n",
    "dataset_country_test = dataset_country_test.prefetch(buffer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 2s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(dataset_country_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0002767175\n"
     ]
    }
   ],
   "source": [
    "# Get the true values from the test dataset\n",
    "true_values = []\n",
    "for _, rate in dataset_country_test:\n",
    "    true_values.extend(rate.numpy())\n",
    "\n",
    "# Convert true_values to a numpy array\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "# Convert predictions to a numpy array if not already\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Compute MSE using TensorFlow\n",
    "mse = tf.reduce_mean(tf.square(true_values - predictions)).numpy()\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Lee-Carter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(470000, 5)\n"
     ]
    }
   ],
   "source": [
    "# non-tensor train / test split (same years in training / test here as in method above)\n",
    "training_index = np.logical_and(data[:, 2] >= 1959, data[:, 2] <= 2005)\n",
    "training_data = data[training_index, :]\n",
    "print(training_data.shape)\n",
    "\n",
    "test_index = np.logical_and(data[:, 2] > 2005, data[:, 2] <= 2015)\n",
    "test_data = data[test_index, :]\n",
    "\n",
    "final_test_index = np.logical_and(data[:, 2] > 2015, data[:, 2] <= 2019)\n",
    "final_test = data[final_test_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up lee-carter function\n",
    "\n",
    "def lee_carter(mx_matrix):\n",
    "    \"\"\"\n",
    "    Run the Lee-Carter model on age-specific mortality data.\n",
    "    \n",
    "    Args:\n",
    "        mx_matrix (numpy.ndarray): A 2D array of age-specific mortality rates. rows = age, columns = years\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the estimated parameters (ax, bx, kt) and the fitted mortality rates.\n",
    "    \"\"\"\n",
    "    mx_matrix[mx_matrix <= 0] = 1e-9\n",
    "\n",
    "    ax = np.mean(np.log(mx_matrix), axis=1)\n",
    "    ax = ax.reshape(-1, 1) # reshape ax into column vector\n",
    "    \n",
    "    centered_mx = np.log(mx_matrix) - ax\n",
    "    \n",
    "    # SVD\n",
    "    U, S, Vt = np.linalg.svd(centered_mx, full_matrices=False)\n",
    "\n",
    "    # extract right and left singular vectors (bx and kt)\n",
    "    bx = U[:, 0]\n",
    "    kt = Vt[0, :]\n",
    "    # print(kt)\n",
    "\n",
    "    # normalize bx and kt \n",
    "    bx = bx / np.sum(bx)\n",
    "    # print(np.mean(kt))\n",
    "    kt = kt - np.mean(kt)\n",
    "    # print(np.mean(kt))\n",
    "\n",
    "    # estimate fitted mortality \n",
    "    fitted_mort = np.exp(ax + np.outer(bx, kt))\n",
    "\n",
    "    return (ax, bx, kt), fitted_mort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up function to run multiple models on all years in training data\n",
    "\n",
    "def lee_carter_state_gender(data):\n",
    "\n",
    "    states = np.unique(data[:, 0])\n",
    "    genders = np.unique(data[:, 1])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for state in states:\n",
    "        for gender in genders:\n",
    "            mask = (data[:, 0] == state) & (data[:, 1] == gender)\n",
    "            state_gender_data = data[mask]\n",
    "\n",
    "            # extract ages and years\n",
    "            years = np.unique(state_gender_data[:, 2])\n",
    "            ages = np.unique(state_gender_data[:, 3])\n",
    "\n",
    "            m_x = np.zeros((len(ages), len(years)))\n",
    "\n",
    "            for i, age in enumerate(ages):\n",
    "                for j, year in enumerate(years):\n",
    "                    mask = (state_gender_data[:, 3] == age) & (state_gender_data[:, 2] == year)\n",
    "                    m_x[i,j] = state_gender_data[mask, 4]\n",
    "\n",
    "            params, fitted_mort = lee_carter(m_x)\n",
    "    \n",
    "            # Store the results for the current state and gender\n",
    "            results[(state, gender)] = {\n",
    "                'params': params,\n",
    "                'fitted_mortality': fitted_mort\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lee_carter_forecast(results, h, start_year, ages, drift=True):\n",
    "    \"\"\"\n",
    "    Perform the forecasting step of the Lee-Carter method using a random walk with drift.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing the estimated parameters (ax, bx, kt) for each state and gender combination.\n",
    "        h (int): The number of future periods to forecast.\n",
    "        start_year (int): The starting year of the forecast.\n",
    "        ages (numpy.ndarray): A 1D array of ages corresponding to the rows of the mortality matrix.\n",
    "        drift (bool, optional): Whether to include a drift term in the random walk. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A 2D array with 5 columns representing state, gender, year, age, and forecasted mortality rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    forecasts = []\n",
    "    \n",
    "    for state, gender in results.keys():\n",
    "        ax, bx, kt = results[(state, gender)]['params']\n",
    "        \n",
    "        # Estimate the drift term\n",
    "        if drift:\n",
    "            drift_term = (kt[-1] - kt[0]) / (len(kt) - 1)\n",
    "        else:\n",
    "            drift_term = 0\n",
    "        \n",
    "        # Forecast future kt values using a random walk with drift\n",
    "        kt_forecast = np.zeros(h)\n",
    "        kt_forecast[0] = kt[-1]\n",
    "        for i in range(1, h):\n",
    "            kt_forecast[i] = kt_forecast[i-1] + drift_term + np.random.normal(0, 1)\n",
    "        \n",
    "        # Forecast future mortality rates\n",
    "        ax_matrix = np.repeat(ax, h).reshape(-1, h)\n",
    "        bx_matrix = np.repeat(bx, h).reshape(-1, h)\n",
    "        kt_matrix = np.repeat(kt_forecast, len(ax)).reshape(h, -1).T\n",
    "        mortality_forecast = np.exp(ax_matrix + bx_matrix * kt_matrix)\n",
    "\n",
    "        # Create a 2D array with state, gender, year, age, and forecasted mortality rate\n",
    "        for i in range(h):\n",
    "            year = start_year + i\n",
    "            for j, age in enumerate(ages):\n",
    "                forecasts.append([state, gender, year, age, mortality_forecast[j, i]])\n",
    "\n",
    "    # Convert forecasts to a NumPy array\n",
    "    forecasts = np.array(forecasts)\n",
    "\n",
    "    # Sort the forecasts array based on the first four columns\n",
    "    sorted_indices = np.lexsort((forecasts[:, 3], forecasts[:, 2], forecasts[:, 1], forecasts[:, 0]))\n",
    "    forecasts = forecasts[sorted_indices]\n",
    "\n",
    "    \n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(forecasted_rates, actual_rates):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) between the forecasted and actual mortality rates.\n",
    "    \n",
    "    Args:\n",
    "        forecasted_rates (numpy.ndarray): A 2D array with 5 columns representing state, gender, year, age, and forecasted mortality rate.\n",
    "        actual_rates (numpy.ndarray): A 2D array with 5 columns representing state, gender, year, age, and actual mortality rate.\n",
    "        \n",
    "    Returns:\n",
    "        float: The Mean Squared Error (MSE) between the forecasted and actual mortality rates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the forecasted and actual mortality rates\n",
    "    forecasted_values = forecasted_rates[:, 4]\n",
    "    actual_values = actual_rates[:, 4]\n",
    "    \n",
    "    # Calculate the squared differences between the forecasted and actual rates\n",
    "    squared_differences = (forecasted_values - actual_values) ** 2\n",
    "    \n",
    "    # Calculate the Mean Squared Error (MSE)\n",
    "    mse = np.mean(squared_differences)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lc_model(train_data, test_data):\n",
    "    lc_output = lee_carter_state_gender(train_data)\n",
    "    predictions = lee_carter_forecast(lc_output, h=10, start_year=2006, ages=range(0, 100))\n",
    "    test_mse = calculate_mse(predictions, test_data)\n",
    "\n",
    "    return np.array(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_results = run_lc_model(train_data=training_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Table 1: Training and Test MSEs\n",
    "This table will document average MSEs (for states alone, countries alone, and total) over 5 training runs with each model (LC, deep learning seperate, deep learning joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(num_iterations):\n",
    "    results = []\n",
    "    for i in range(num_iterations):\n",
    "        lc_results = run_lc_model(train_data=training_data, test_data=test_data)\n",
    "        deep_results = run_deep_model(dataset_train=dataset_train, dataset_test=dataset_test)\n",
    "        results.append((deep_results, lc_results))\n",
    "        print(f\"Loop {i}: deep mse {results[i][0]} & lc mse {results[i][1]}\")\n",
    "\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/wn6shwbs4_9gcwtsrgz8v5vc0000gn/T/ipykernel_88639/1271048323.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  m_x[i,j] = state_gender_data[mask, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 0: deep mse 4.0398939745500684e-05 & lc mse 0.0003915247299684933\n",
      "Loop 1: deep mse 6.0458645748440176e-05 & lc mse 0.00039128320488239905\n",
      "Loop 2: deep mse 4.817579247173853e-05 & lc mse 0.0003891320282911292\n",
      "Loop 3: deep mse 5.007610161555931e-05 & lc mse 0.0003862197077289283\n",
      "Loop 4: deep mse 4.09356398449745e-05 & lc mse 0.00038688559897930323\n",
      "Loop 5: deep mse 4.911636278848164e-05 & lc mse 0.0003864877662411712\n",
      "Loop 6: deep mse 4.301398075767793e-05 & lc mse 0.0003892479426403891\n",
      "Loop 7: deep mse 4.956285556545481e-05 & lc mse 0.00038826033560748026\n",
      "Loop 8: deep mse 4.017120591015555e-05 & lc mse 0.0003908170289735953\n",
      "Loop 9: deep mse 5.103383955429308e-05 & lc mse 0.00038729158184310007\n",
      "Loop 10: deep mse 4.477219408727251e-05 & lc mse 0.0003893728425534582\n",
      "Loop 11: deep mse 4.873637590208091e-05 & lc mse 0.00039045673061155\n",
      "Loop 12: deep mse 4.5513785153161734e-05 & lc mse 0.0003895944230450417\n",
      "Loop 13: deep mse 7.137803186196834e-05 & lc mse 0.00038838044018389434\n",
      "Loop 14: deep mse 5.818564386572689e-05 & lc mse 0.00038506183491556774\n",
      "Loop 15: deep mse 5.986831820337102e-05 & lc mse 0.0003870977087485554\n",
      "Loop 16: deep mse 4.108035864192061e-05 & lc mse 0.0003863712150338229\n",
      "Loop 17: deep mse 4.370341048343107e-05 & lc mse 0.00039019770114330634\n",
      "Loop 18: deep mse 4.055399767821655e-05 & lc mse 0.00039094099481334026\n",
      "Loop 19: deep mse 4.116302807233296e-05 & lc mse 0.00039022104129748535\n",
      "Loop 20: deep mse 4.791400351678021e-05 & lc mse 0.00038905815260852994\n",
      "Loop 21: deep mse 4.290840297471732e-05 & lc mse 0.00038547200411755784\n",
      "Loop 22: deep mse 6.165607192087919e-05 & lc mse 0.00038458043220354626\n",
      "Loop 23: deep mse 5.2702234825119376e-05 & lc mse 0.0003919180677142061\n",
      "Loop 24: deep mse 4.242498835083097e-05 & lc mse 0.00038749852383906686\n",
      "Loop 25: deep mse 4.239140616846271e-05 & lc mse 0.000386309489546373\n",
      "Loop 26: deep mse 6.20987921138294e-05 & lc mse 0.000388732004985983\n",
      "Loop 27: deep mse 4.934299431624822e-05 & lc mse 0.0003916216373170008\n",
      "Loop 28: deep mse 4.587364674080163e-05 & lc mse 0.000389226141050393\n",
      "Loop 29: deep mse 4.513711610343307e-05 & lc mse 0.00038740700920695306\n"
     ]
    }
   ],
   "source": [
    "comparison_results = compare_models(num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_results_np = np.array(comparison_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
